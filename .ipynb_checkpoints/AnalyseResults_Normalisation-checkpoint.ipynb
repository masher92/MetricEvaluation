{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5e79b1a7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def safe_log_transform(df, columns, skew_threshold=1.0):\n",
    "    \"\"\"\n",
    "    For each metric (and its _DMC_10 counterpart), checks skewness and applies either:\n",
    "    - log1p (if both are positive-only)\n",
    "    - Yeo-Johnson (if either includes 0 or negative values)\n",
    "\n",
    "    Only applies a transform if *either version* is skewed beyond threshold.\n",
    "\n",
    "    Returns:\n",
    "        transformed_df: df with added transformed columns\n",
    "    \"\"\"\n",
    "    transformed_df = df.copy()\n",
    "    checked = set()\n",
    "\n",
    "    for col in columns:\n",
    "        if col in checked:\n",
    "            continue  # already processed as part of a pair\n",
    "\n",
    "        dmc_col = col + '_DMC_10'\n",
    "        pair = [col]\n",
    "        if dmc_col in df.columns:\n",
    "            pair.append(dmc_col)\n",
    "\n",
    "        # Check skewness across the pair\n",
    "        skews = {c: skew(df[c].dropna()) for c in pair}\n",
    "        max_skew = max(abs(s) for s in skews.values())\n",
    "\n",
    "        if max_skew <= skew_threshold:\n",
    "            continue  # Skip transformation — not skewed enough\n",
    "\n",
    "        # Determine appropriate transformation\n",
    "        all_positive = all((df[c] > 0).all() for c in pair)\n",
    "        if all_positive:\n",
    "            for c in pair:\n",
    "                transformed_df[c + '_log'] = np.log1p(df[c])\n",
    "        else:\n",
    "            transformer = PowerTransformer(method='yeo-johnson', standardize=False)\n",
    "            for c in pair:\n",
    "                transformed_vals = transformer.fit_transform(df[c].values.reshape(-1, 1)).flatten()\n",
    "                transformed_df[c + '_yj'] = transformed_vals\n",
    "        checked.update(pair)\n",
    "\n",
    "    return transformed_df\n",
    "\n",
    "\n",
    "def plot_metric_histograms(df, columns=None, bins=30, skew_threshold=1.0):\n",
    "    \"\"\"\n",
    "    Plots histograms for each metric column with skewness displayed.\n",
    "    If no columns provided, uses all numeric columns.\n",
    "    \"\"\"\n",
    "    if columns is None:\n",
    "        columns = df.select_dtypes(include='number').columns\n",
    "\n",
    "    n_cols = 3\n",
    "    n_rows = int(np.ceil(len(columns) / n_cols))\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols*3, n_rows*2))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, col in enumerate(columns):\n",
    "        data = df[col].dropna()\n",
    "        sk = skew(data)\n",
    "        sns.histplot(data, bins=bins, kde=True, ax=axes[i])\n",
    "        axes[i].set_title(f\"{col}\\nSkewness: {sk:.2f}\")\n",
    "        axes[i].set_xlabel('')\n",
    "        axes[i].set_ylabel('')\n",
    "\n",
    "    # Turn off unused axes\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        axes[j].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_original_vs_transformed(df, original_columns):\n",
    "    \"\"\"\n",
    "    Plots side-by-side histograms of original and transformed versions of each metric.\n",
    "    Transformed versions are expected to be in columns named '<original>_log' or '<original>_yj'.\n",
    "    \"\"\"\n",
    "    for col in original_columns:\n",
    "        transformed_col = None\n",
    "        if col + '_log' in df.columns:\n",
    "            transformed_col = col + '_log'\n",
    "        elif col + '_yj' in df.columns:\n",
    "            transformed_col = col + '_yj'\n",
    "        else:\n",
    "            continue  # Skip if no transformed version exists\n",
    "\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "        # Original\n",
    "        sns.histplot(df[col].dropna(), bins=30, kde=True, ax=axes[0])\n",
    "        orig_skew = skew(df[col].dropna())\n",
    "        axes[0].set_title(f\"Original: {col}\\nSkewness: {orig_skew:.2f}\")\n",
    "        axes[0].set_ylabel(\"Frequency\")\n",
    "\n",
    "        # Transformed\n",
    "        sns.histplot(df[transformed_col].dropna(), bins=30, kde=True, ax=axes[1])\n",
    "        trans_skew = skew(df[transformed_col].dropna())\n",
    "        axes[1].set_title(f\"Transformed: {transformed_col}\\nSkewness: {trans_skew:.2f}\")\n",
    "        axes[1].set_ylabel(\"Frequency\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "def select_best_transformed_version(df):\n",
    "    selected_cols = []\n",
    "\n",
    "    # Get all base column names (before _log or _yj)\n",
    "    base_cols = set(col.replace('_log', '').replace('_yj', '') for col in df.columns)\n",
    "\n",
    "    for base in base_cols:\n",
    "        log_col = base + '_log'\n",
    "        yj_col = base + '_yj'\n",
    "\n",
    "        if log_col in df.columns:\n",
    "            selected_cols.append(log_col)\n",
    "        elif yj_col in df.columns:\n",
    "            selected_cols.append(yj_col)\n",
    "        elif base in df.columns:\n",
    "            selected_cols.append(base)\n",
    "        # else: skip — no usable version of this metric\n",
    "\n",
    "    return df[selected_cols].copy()\n",
    "\n",
    "\n",
    "def safe_log_transform_old(df, columns, skew_threshold=1.0):\n",
    "    \"\"\"\n",
    "    Applies log1p transformation to highly skewed, positive-only columns.\n",
    "    For mixed or negative columns, applies Yeo-Johnson transformation instead.\n",
    "\n",
    "    Returns a new DataFrame with log-transformed columns (originals unchanged).\n",
    "    \"\"\"\n",
    "    from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "    transformed_df = df.copy()\n",
    "    skew_vals = df[columns].apply(skew)\n",
    "\n",
    "    for col in skew_vals.index:\n",
    "        if abs(skew_vals[col]) > skew_threshold:\n",
    "            col_data = df[col]\n",
    "            if (col_data <= 0).any():\n",
    "                # Mixed or negative values: use Yeo-Johnson\n",
    "                pt = PowerTransformer(method='yeo-johnson', standardize=False)\n",
    "                transformed = pt.fit_transform(col_data.values.reshape(-1, 1)).flatten()\n",
    "                transformed_df[col + '_yj'] = transformed\n",
    "            else:\n",
    "                # Positive-only: safe to use log1p\n",
    "                transformed_df[col + '_log'] = np.log1p(col_data)\n",
    "    \n",
    "    return transformed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ff30609b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from scipy.interpolate import interp1d\n",
    "import seaborn as sns\n",
    "import os\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import pickle\n",
    "from scipy.stats import ttest_rel, wilcoxon, pearsonr, spearmanr, kendalltau\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler  \n",
    "from scipy.stats import skew\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "from PlottingFunctions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98f1e8c",
   "metadata": {},
   "source": [
    "## Read dataframe of results for all gauges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4d46c891",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/a321/gy17m2a/anaconda_install/anaconda3/envs/ukcp18/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3457: DtypeWarning: Columns (143) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "/nfs/a321/gy17m2a/anaconda_install/anaconda3/envs/ukcp18/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3457: DtypeWarning: Columns (83,143) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "all_events =[]\n",
    "for file in os.listdir(\"../DanishRainData_Outputs/5mins/\"):\n",
    "    df = pd.read_csv(f\"../DanishRainData_Outputs/5mins/{file}\")\n",
    "    df['event_num']=range(0, len(df))\n",
    "    all_events.append(df)\n",
    "all_events_df = pd.concat(all_events)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "384c937e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_events_df.rename(columns={'4th_w_peak':\"Huff quartile\"}, inplace=True)\n",
    "all_events_df.rename(columns={'4th_w_peak_DMC_10':\"Huff quartile_DMC_10\"}, inplace=True)\n",
    "all_events_df.rename(columns={'3rd_w_peak':\"Huff third\"}, inplace=True)\n",
    "all_events_df.rename(columns={'3rd_w_peak_DMC_10':\"Huff third_DMC_10\"}, inplace=True)\n",
    "all_events_df.rename(columns={'5th_w_peak':\"Huff quintile\"}, inplace=True)\n",
    "all_events_df.rename(columns={'5th_w_peak_DMC_10':\"Huff quintile_DMC_10\"}, inplace=True)\n",
    "# all_events_df.rename(columns={'3rd_ARR':\"3rd (w/ D50)\"}, inplace=True)\n",
    "# all_events_df.rename(columns={'3rd_ARR_DMC_10':\"3rd (w/ D50)_DMC_10\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a96de8",
   "metadata": {},
   "source": [
    "### Remove events which are too small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f2347633",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "539546\n",
      "516453\n"
     ]
    }
   ],
   "source": [
    "print(len(all_events_df))\n",
    "all_events_df = all_events_df[all_events_df['total_precip']>4].copy()\n",
    "print(len(all_events_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702c0e33",
   "metadata": {},
   "source": [
    "### Remove normalisations we're not interested in (rn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b4fd903c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189\n",
      "129\n"
     ]
    }
   ],
   "source": [
    "print(len(all_events_df.columns))\n",
    "columns_to_drop = [col for col in all_events_df.columns if '_norm' in col or '_dblnorm' in col or 'DMC_100' in col]\n",
    "all_events_df.drop(columns=columns_to_drop, inplace=True)\n",
    "print(len(all_events_df.columns))\n",
    "all_events_df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0ba127",
   "metadata": {},
   "source": [
    "### Remove columns we're not interested in (rn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7a658b9d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "del all_events_df['BSC'], all_events_df['BSC_DMC_10']\n",
    "del all_events_df['duration']\n",
    "del all_events_df['total_precip']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1cc25b",
   "metadata": {},
   "source": [
    "### Delete problematic (np.nan) columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5d025804",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cols_to_keep = all_events_df.columns\n",
    "cols_to_del = ['min_intensity', 'min_intensity_DMC_10', \n",
    "               'min_intensity', 'min_intensity_DMC_10', # doesnt make sense, as for DMC it is always 0.1\n",
    "               'frac_q1', 'frac_q2', 'frac_q3', 'frac_q4', ## loads have NAN when we don't interpolate\n",
    "               'frac_q1_DMC_10', 'frac_q2_DMC_10', 'frac_q3_DMC_10', 'frac_q4_DMC_10',   ## loads have NAN when we don't interpolate\n",
    "              'heaviest_half', 'heaviest_half_DMC_10', # lots of events dont have a 'run' over threshold\n",
    "#               '3rd_com',  '3rd_com_DMC_10',  # this metric doesnt calculate properly\n",
    "              ]\n",
    "#                '3rd_rcg','3rd_rcg_DMC_10',]\n",
    "\n",
    "cols_to_keep = [x for x in cols_to_keep if x not in cols_to_del]  \n",
    "\n",
    "all_events_df = all_events_df[cols_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9d742b39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# bad_indices = all_events_df[all_events_df['third_ppr_DMC_10'].isnull()].index\n",
    "# all_events_df[all_events_df['third_ppr_DMC_10'].isnull()][['third_ppr_DMC_10', 'third_ppr']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bf9a608d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with at least one NaN value:\n",
      "['asymm_d', 'Event Loading', 'lorentz_asymetry', 'Event Loading_DMC_10', 'lorentz_asymetry_DMC_10']\n"
     ]
    }
   ],
   "source": [
    "# Find columns with nan for 60 mins\n",
    "columns_with_nan = all_events_df.columns[all_events_df.isnull().any()].tolist()\n",
    "print(\"Columns with at least one NaN value:\")\n",
    "print(columns_with_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "310ddf83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "Columns with at least one NaN value:\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# # Find indices of rows with any NaNs in the 60-minute data\n",
    "bad_indices = all_events_df[all_events_df.isnull().any(axis=1)].index\n",
    "print(len(bad_indices))\n",
    "# Drop rows at these indices from all dataframes\n",
    "all_events_df = all_events_df.drop(index=bad_indices)\n",
    "\n",
    "del all_events_df['event_num']\n",
    "all_events_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Recheck for NANS\n",
    "columns_with_nan = all_events_df.columns[all_events_df.isnull().any()].tolist()\n",
    "print(\"Columns with at least one NaN value:\")\n",
    "print(columns_with_nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f5450c",
   "metadata": {},
   "source": [
    "## Check skewness of variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1da1ce80",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "metric_columns = all_events_df.columns\n",
    "metric_columns = metric_columns.drop(['gauge_num', \"start_time\", \"end_time\"])\n",
    "all_events_df = all_events_df[metric_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "20e272cf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot_metric_histograms(all_events_df, columns=log_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fdff6c0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transformed_all_events_df = safe_log_transform(all_events_df, metric_columns)\n",
    "transformed_all_events_df2 = select_best_transformed_version(transformed_all_events_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e0fbffbf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# original_metric_columns = [col for col in all_events_df.columns \n",
    "#                            if not (col.endswith('_log') or col.endswith('_yj')) \n",
    "#                            and all_events_df[col].dtype.kind in 'fi']  # float/int only\n",
    "\n",
    "# plot_original_vs_transformed(transformed_all_events_df, original_metric_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854b237b",
   "metadata": {},
   "source": [
    "## Standardise Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a1547a52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metric_columns = transformed_all_events_df2.columns\n",
    "standard_scaler = MinMaxScaler()\n",
    "transformed_scaled = standard_scaler.fit_transform(transformed_all_events_df2[metric_columns])\n",
    "# # Convert scaled values back to DataFrame and concatenate with original non-numeric columns\n",
    "transformed_scaled = pd.DataFrame(transformed_scaled, columns=metric_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "663d0e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric_columns = all_events_df.columns\n",
    "# standard_scaler = MinMaxScaler()\n",
    "# original_scaled = standard_scaler.fit_transform(all_events_df[metric_columns])\n",
    "\n",
    "# # Convert scaled values back to DataFrame and concatenate with original non-numeric columns\n",
    "# original_scaled = pd.DataFrame(original_scaled, columns=metric_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f1a65e04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metric_base_names = {col.replace(suffix, '') for col in all_events_df.columns  for suffix in ['_DMC_10']  if col.endswith(suffix)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8ab684c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pearsonsr_ls = []\n",
    "spearmansr_ls = []\n",
    "metric_names = []\n",
    "suffixes = ['', '_log', '_yj']\n",
    "\n",
    "for i, base_name in enumerate(metric_base_names):\n",
    "    found_pair = False\n",
    "    for suffix in suffixes:\n",
    "        col1 = base_name + suffix\n",
    "        col2 = base_name + '_DMC_10' + suffix\n",
    "\n",
    "        if col1 in transformed_scaled.columns and col2 in transformed_scaled.columns:\n",
    "            x = transformed_scaled[col1]\n",
    "            y = transformed_scaled[col2]\n",
    "            \n",
    "            pearsonsr_ls.append(pearsonr(x, y)[0])\n",
    "            spearmansr_ls.append(spearmanr(x, y)[0])\n",
    "            metric_names.append(base_name + suffix)\n",
    "            found_pair = True\n",
    "            break  # stop after finding first matching suffix pair\n",
    "            \n",
    "    if not found_pair:\n",
    "        print(f\"No matching pair found for metric '{base_name}' with any suffix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9ef5201b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metric_scores = pd.DataFrame({'Metric':metric_names,\n",
    "                             \"pearsonsr\": pearsonsr_ls, \"spearmansr\": spearmansr_ls})\n",
    "metric_scores.sort_values(by=\"pearsonsr\", ascending=False, inplace=True)\n",
    "sorted_metrics = metric_scores.sort_values(by=\"pearsonsr\", ascending=False)['Metric'].values\n",
    "cleaned_sorted_metrics = [m.replace('_log', '').replace('_yj', '') for m in sorted_metrics]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6145239",
   "metadata": {},
   "source": [
    "## Create correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "63fb148d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # Define suffixes to consider, ordered by preference\n",
    "# suffixes = ['', '_log', '_yj']\n",
    "\n",
    "# n_metrics = len(metric_base_names)\n",
    "# n_cols = 6\n",
    "# n_rows = int(np.ceil(n_metrics / n_cols))\n",
    "\n",
    "# fig, axs = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(6 * n_cols, 4.5 * n_rows), sharex=True, sharey=True)\n",
    "# axs = axs.flatten()\n",
    "\n",
    "# for i, base_name in enumerate(cleaned_sorted_metrics):\n",
    "    \n",
    "#     if base_name == 'Huff quartile':\n",
    "#         bins=4\n",
    "#     elif base_name == 'Huff quintile':\n",
    "#         bins=5   \n",
    "#     elif base_name == 'Huff third' or base_name =='3rd (w/ D50)' or base_name == '3rd_rcg' or base_name == '3rd_ARR':\n",
    "#         bins=3\n",
    "#     elif base_name == 'BSC_Index':\n",
    "#         bins=5\n",
    "#     else:\n",
    "#         bins=30\n",
    "    \n",
    "#     found_pair = False\n",
    "#     for suffix in suffixes:\n",
    "#         col1 = base_name + suffix\n",
    "#         col2 = base_name + '_DMC_10' + suffix\n",
    "\n",
    "#         if col1 in scaled_all_events_df.columns and col2 in scaled_all_events_df.columns:\n",
    "#             ax = axs[i]\n",
    "#             x = scaled_all_events_df[col1]\n",
    "#             y = scaled_all_events_df[col2]\n",
    "\n",
    "#             sns.histplot(x=x, y=y, bins=bins, pmax=0.9, ax=ax, cmap=\"viridis\", cbar=False)\n",
    "#             ax.set_title(f\"{base_name} {suffix if suffix else '(raw)'}\", fontsize=30)\n",
    "#             ax.set_xlabel(\"Raw\")\n",
    "#             ax.set_ylabel(\"DMC\")\n",
    "# #             ax.set_title(f\"{base_name}_{suffix}\", fontsize=30)\n",
    "#             found_pair = True\n",
    "            \n",
    "#             min_val = min(np.min(x), np.min(y))\n",
    "#             max_val = max(np.max(x), np.max(y))\n",
    "#             ax.plot([min_val, max_val], [min_val, max_val], color='black', linestyle='solid', linewidth=3.5, label='1:1 Line')\n",
    "            \n",
    "#             pearsons_r = pearsonr(x, y)[0]\n",
    "#             spearmans_r = spearmanr(x, y)[0]\n",
    "            \n",
    "#             ax.annotate(f\"{pearsons_r:.3f}\", \n",
    "#                     xy=(0.80, 0.1), xycoords='axes fraction',\n",
    "#                     fontsize=25, color='black', \n",
    "#                     # ha='right', va='bottom',\n",
    "#                     bbox=dict(facecolor='white', edgecolor='black', boxstyle='round,pad=0.5'))\n",
    "#             ax.annotate(f\"{spearmans_r:.3f}\", \n",
    "#                     xy=(0.06, 0.86), xycoords='axes fraction',\n",
    "#                     fontsize=25, color='black', \n",
    "#                     # ha='right', va='bottom',\n",
    "#                     bbox=dict(facecolor='white', edgecolor='black', boxstyle='round,pad=0.5'))    \n",
    "            \n",
    "#             ax.tick_params(axis='both', which='major', labelsize=25)\n",
    "#             ax.xaxis.label.set_size(25)  # Increase x-axis label font size\n",
    "#             ax.yaxis.label.set_size(25)  # Increase y-axis label font size\n",
    "            \n",
    "#             break  # stop after finding first matching suffix pair\n",
    "            \n",
    "#     if not found_pair:\n",
    "#         print(f\"No matching pair found for metric '{base_name}' with any suffix\")\n",
    "\n",
    "# # Remove unused axes if any\n",
    "# for j in range(i + 1, len(axs)):\n",
    "#     fig.delaxes(axs[j])\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0505fb",
   "metadata": {},
   "source": [
    "### Plot Bland Altman Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d280ed9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot_bland_altman_grid(bland_altman_data, plots_per_row=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91450c0b",
   "metadata": {},
   "source": [
    "## Cohen's D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3cfa67ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "def cohens_d(x, y):\n",
    "    n1, n2 = len(x), len(y)\n",
    "    s1, s2 = np.std(x, ddof=1), np.std(y, ddof=1)\n",
    "    s_pooled = np.sqrt(((n1 -1)*s1**2 + (n2 -1)*s2**2) / (n1 + n2 -2))\n",
    "    d = (np.mean(x) - np.mean(y)) / s_pooled\n",
    "    d=abs(d)\n",
    "    if d<=0.2:\n",
    "        meaning = \"Negligible\"\n",
    "    elif d>0.2 and d <=0.5:\n",
    "        meaning=\"Small\"\n",
    "    elif d >0.5 and d <= 0.8:\n",
    "        meaning = 'Moderate'\n",
    "    elif d>0.8:\n",
    "        meaning = 'Large'\n",
    "    return d, meaning\n",
    "\n",
    "def instability_index(x, y):\n",
    "    rho, _ = spearmanr(x, y)\n",
    "    instability_index = 1 - rho\n",
    "    \n",
    "    if instability_index<=0.05:\n",
    "        meaning = \"Very low\" # \"Very stable\"\n",
    "    elif instability_index>0.05 and instability_index <=0.15:\n",
    "        meaning= \"Low\"# \"Stable\"\n",
    "    elif instability_index >0.15 and instability_index <= 0.35:\n",
    "        meaning = 'Moderate' # 'Moderate instability'\n",
    "    elif instability_index >0.35 and instability_index <= 0.60:\n",
    "        meaning = \"High\" # 'High instability'\n",
    "    elif instability_index>0.6:\n",
    "        meaning = \"Very high\" # 'Very unstable'\n",
    "    \n",
    "    return instability_index, meaning\n",
    "\n",
    "def cliffs_delta(x, y):\n",
    "    from itertools import product\n",
    "    x, y = np.asarray(x), np.asarray(y)\n",
    "    n_x, n_y = len(x), len(y)\n",
    "    greater = sum(1 for a, b in product(x, y) if a > b)\n",
    "    less = sum(1 for a, b in product(x, y) if a < b)\n",
    "    delta = (greater - less) / (n_x * n_y)\n",
    "    abs_delta = abs(delta)\n",
    "    \n",
    "    if abs_delta < 0.147:\n",
    "        meaning = \"Negligible\"\n",
    "    elif abs_delta < 0.33:\n",
    "        meaning = \"Small\"\n",
    "    elif abs_delta < 0.474:\n",
    "        meaning = \"Medium\"\n",
    "    else:\n",
    "        meaning = \"Large\"\n",
    "    \n",
    "    return abs_delta, meaning\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "def cliffs_delta_fast(x, y):\n",
    "    x = np.asarray(x)\n",
    "    y = np.asarray(y)\n",
    "    n_x = len(x)\n",
    "    n_y = len(y)\n",
    "\n",
    "    # Mann-Whitney U test (two-sided, but we just use U statistic)\n",
    "    U, _ = mannwhitneyu(x, y, alternative='two-sided')\n",
    "    # Compute delta\n",
    "    delta = (2 * U) / (n_x * n_y) - 1\n",
    "    abs_delta = abs(delta)\n",
    "\n",
    "    if abs_delta < 0.147:\n",
    "        meaning = \"Negligible\"\n",
    "    elif abs_delta < 0.33:\n",
    "        meaning = \"Small\"\n",
    "    elif abs_delta < 0.474:\n",
    "        meaning = \"Medium\"\n",
    "    else:\n",
    "        meaning = \"Large\"\n",
    "    \n",
    "    return abs_delta, meaning\n",
    "\n",
    "\n",
    "from scipy.stats import kendalltau\n",
    "\n",
    "def ordinal_ranking_instability(x, y):\n",
    "    tau, _ = kendalltau(x, y)\n",
    "    instability = 1 - tau\n",
    "    if instability <= 0.05:\n",
    "        meaning = \"Very low\"\n",
    "    elif instability <= 0.15:\n",
    "        meaning = \"Low\"\n",
    "    elif instability <= 0.35:\n",
    "        meaning = \"Moderate\"\n",
    "    elif instability <= 0.60:\n",
    "        meaning = \"High\"\n",
    "    else:\n",
    "        meaning = \"Very high\"\n",
    "    return instability, meaning\n",
    "\n",
    "\n",
    "# # Example for one metric\n",
    "# for metric in cleaned_sorted_metrics:\n",
    "#     metric_fine = all_events_df[metric]\n",
    "#     metric_coarse = all_events_df[f'{metric}_DMC_10']\n",
    "\n",
    "#     d, meaning = cohens_d(metric_fine, metric_coarse)\n",
    "#     instab, translation = instability_index(metric_fine, metric_coarse)\n",
    "\n",
    "# #     print(f\"{metric}: Cohen's d: {d:.2f}, is {meaning}. Instability is: {instab:.2f} \")\n",
    "#     print(f\"{metric}: \\n Sensitivity of event rankings to DMC transformation (Cohen's D): {meaning} ({d:.2f}).  \\n Sensitivity of event numerical meanings to DMC transformation (instability_index): {translation} ({instab:.2f}) \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "39b07309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected categorical metrics: ['third_ppr', 'Huff quartile', '3rd_ARR', 'Huff third', 'Huff quintile', '3rd_rcg', 'BSC_Index']\n"
     ]
    }
   ],
   "source": [
    "categorical_metric_list = []\n",
    "\n",
    "for metric in cleaned_sorted_metrics:\n",
    "    unique_vals = all_events_df[metric].dropna().unique()\n",
    "    if len(unique_vals) < 6:\n",
    "        categorical_metric_list.append(metric)\n",
    "\n",
    "print(\"Detected categorical metrics:\", categorical_metric_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33971ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r\"\\begin{tabular}{llll}\")\n",
    "print(r\"\\hline\")\n",
    "print(r\"Metric & Metric type & Ranking Instability & Numerical Instability \\\\\")\n",
    "print(r\"\\hline\")\n",
    "\n",
    "for metric in cleaned_sorted_metrics:\n",
    "    x = all_events_df[metric]\n",
    "    y = all_events_df[f'{metric}_DMC_10']\n",
    "\n",
    "    if metric in categorical_metric_list:\n",
    "        instab_rank, meaning_rank = ordinal_ranking_instability(x, y)\n",
    "        delta, meaning_num = cliffs_delta_fast(x, y)\n",
    "        metric_type = 'categorical'\n",
    "        print(f\"{metric} & {metric_type} & {meaning_rank} ({instab_rank:.2f}) & {meaning_num} ({delta:.2f}) \\\\\\\\\")\n",
    "    else:\n",
    "        d, meaning = cohens_d(x, y)\n",
    "        instab, translation = instability_index(x, y)\n",
    "        metric_type = 'continuous'\n",
    "        print(f\"{metric} & {metric_type} & {meaning} ({d:.2f}) & {translation} ({instab:.2f}) \\\\\\\\\")\n",
    "\n",
    "print(r\"\\hline\")\n",
    "print(r\"\\end{tabular}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c86b9b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{llll}\n",
      "\\hline\n",
      "Metric & Ranking Instability & Numerical Instability \\\\\n",
      "\\hline\n",
      "third_ppr & Very low (0.00) & Negligible (0.00) \\\\\n",
      "m3_wi & Negligible (0.00) & Very low (0.00) \\\\\n",
      "time_skewness & Negligible (0.03) & Very low (0.00) \\\\\n",
      "time_kurtosis & Negligible (0.02) & Very low (0.00) \\\\\n",
      "Huff quartile & Very low (0.01) & Negligible (0.00) \\\\\n",
      "T75 & Negligible (0.01) & Very low (0.01) \\\\\n",
      "D50 & Negligible (0.00) & Very low (0.01) \\\\\n",
      "T50 & Negligible (0.00) & Very low (0.01) \\\\\n",
      "centre_gravity_interpolated & Small (0.27) & Very low (0.01) \\\\\n",
      "m4_wi & Negligible (0.00) & Very low (0.01) \\\\\n",
      "m5_wi & Negligible (0.00) & Very low (0.01) \\\\\n",
      "centre_gravity & Negligible (0.01) & Very low (0.03) \\\\\n",
      "m5 & Negligible (0.13) & Very low (0.03) \\\\\n",
      "3rd_ARR & Very low (0.04) & Negligible (0.00) \\\\\n",
      "TCI & Negligible (0.07) & Very low (0.04) \\\\\n",
      "Huff third & Very low (0.05) & Negligible (0.00) \\\\\n",
      "T25 & Negligible (0.00) & Very low (0.02) \\\\\n",
      "Huff quintile & Low (0.07) & Negligible (0.00) \\\\\n",
      "m4 & Small (0.22) & Low (0.06) \\\\\n",
      "m3 & Small (0.35) & Low (0.10) \\\\\n",
      "gini & Moderate (0.50) & Low (0.13) \\\\\n",
      "frac_q1_wi_ & Small (0.25) & Low (0.14) \\\\\n",
      "frac_q4_wi_ & Negligible (0.02) & Moderate (0.19) \\\\\n",
      "cv & Moderate (0.70) & Low (0.15) \\\\\n",
      "frac_q2_wi_ & Negligible (0.20) & Moderate (0.16) \\\\\n",
      "frac_q3_wi_ & Negligible (0.13) & Moderate (0.16) \\\\\n",
      "3rd_rcg & Moderate (0.20) & Negligible (0.10) \\\\\n",
      "event_dry_ratio & Moderate (0.70) & Moderate (0.16) \\\\\n",
      "% rain HIZ & Small (0.22) & Moderate (0.33) \\\\\n",
      "relative_amp & Large (0.86) & Moderate (0.24) \\\\\n",
      "peak_mean_ratio & Large (0.86) & Moderate (0.26) \\\\\n",
      "ni & Large (0.86) & Moderate (0.26) \\\\\n",
      "peak_mean_ratio_scaled & Large (0.86) & Moderate (0.26) \\\\\n",
      "skewp & Negligible (0.20) & High (0.35) \\\\\n",
      "lorentz_asymetry & Small (0.23) & High (0.36) \\\\\n",
      "% time HIZ & Moderate (0.59) & High (0.37) \\\\\n",
      "% time LIZ & Moderate (0.59) & High (0.38) \\\\\n",
      "Event Loading & Small (0.44) & High (0.39) \\\\\n",
      "skewness & Large (0.93) & High (0.38) \\\\\n",
      "intermittency & Large (0.87) & Moderate (0.34) \\\\\n",
      "NRMSE_P & Large (1.08) & High (0.47) \\\\\n",
      "kurtosis & Moderate (0.70) & High (0.54) \\\\\n",
      "m2_wi & Large (1.47) & Very high (0.63) \\\\\n",
      "m2 & Large (1.47) & Very high (0.63) \\\\\n",
      "max_intensity & Large (0.91) & High (0.57) \\\\\n",
      "asymm_d & Negligible (0.07) & Very high (0.62) \\\\\n",
      "m1_wi & Small (0.22) & High (0.58) \\\\\n",
      "m1 & Small (0.22) & High (0.58) \\\\\n",
      "PCI & Large (1.52) & Very high (0.73) \\\\\n",
      "std & Large (0.85) & Very high (0.69) \\\\\n",
      "Mean Intensity HIZ & Moderate (0.75) & Very high (0.77) \\\\\n",
      "mean_intensity & Moderate (0.75) & Very high (1.00) \\\\\n",
      "BSC_Index & Very high (1.01) & Medium (0.42) \\\\\n",
      "\\hline\n",
      "\\end{tabular}\n"
     ]
    }
   ],
   "source": [
    "print(r\"\\begin{tabular}{llll}\")\n",
    "print(r\"\\hline\")\n",
    "print(r\"Metric & Ranking Instability & Numerical Instability \\\\\")\n",
    "print(r\"\\hline\")\n",
    "\n",
    "for metric in cleaned_sorted_metrics:\n",
    "    x = all_events_df[metric]\n",
    "    y = all_events_df[f'{metric}_DMC_10']\n",
    "\n",
    "    if metric in categorical_metric_list:\n",
    "        instab_rank, meaning_rank = ordinal_ranking_instability(x, y)\n",
    "        delta, meaning_num = cliffs_delta_fast(x, y)\n",
    "        print(f\"{metric} & {meaning_rank} ({instab_rank:.2f}) & {meaning_num} ({delta:.2f}) \\\\\\\\\")\n",
    "    else:\n",
    "        d, meaning = cohens_d(x, y)\n",
    "        instab, translation = instability_index(x, y)\n",
    "        print(f\"{metric} & {meaning} ({d:.2f}) & {translation} ({instab:.2f}) \\\\\\\\\")\n",
    "\n",
    "print(r\"\\hline\")\n",
    "print(r\"\\end{tabular}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b8e7c9e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "third_ppr:\n",
      "  Ranking Instability (Kendall's τ): Very low (0.00)\n",
      "  Numerical Instability (Cliff’s Delta): Negligible (0.00)\n",
      "\n",
      "m3_wi:\n",
      "BSC_Index: \n",
      " Sensitivity of event rankings to DMC transformation: Negligible.  \n",
      " Sensitivity of event numerical meanings to DMC transformation: Very low \n",
      "time_skewness:\n",
      "BSC_Index: \n",
      " Sensitivity of event rankings to DMC transformation: Negligible.  \n",
      " Sensitivity of event numerical meanings to DMC transformation: Very low \n",
      "time_kurtosis:\n",
      "BSC_Index: \n",
      " Sensitivity of event rankings to DMC transformation: Negligible.  \n",
      " Sensitivity of event numerical meanings to DMC transformation: Very low \n",
      "Huff quartile:\n",
      "  Ranking Instability (Kendall's τ): Very low (0.01)\n",
      "  Numerical Instability (Cliff’s Delta): Negligible (0.00)\n",
      "\n",
      "T75:\n",
      "BSC_Index: \n",
      " Sensitivity of event rankings to DMC transformation: Negligible.  \n",
      " Sensitivity of event numerical meanings to DMC transformation: Very low \n",
      "D50:\n",
      "BSC_Index: \n",
      " Sensitivity of event rankings to DMC transformation: Negligible.  \n",
      " Sensitivity of event numerical meanings to DMC transformation: Very low \n",
      "T50:\n",
      "BSC_Index: \n",
      " Sensitivity of event rankings to DMC transformation: Negligible.  \n",
      " Sensitivity of event numerical meanings to DMC transformation: Very low \n",
      "centre_gravity_interpolated:\n",
      "BSC_Index: \n",
      " Sensitivity of event rankings to DMC transformation: Small.  \n",
      " Sensitivity of event numerical meanings to DMC transformation: Very low \n",
      "m4_wi:\n",
      "BSC_Index: \n",
      " Sensitivity of event rankings to DMC transformation: Negligible.  \n",
      " Sensitivity of event numerical meanings to DMC transformation: Very low \n",
      "m5_wi:\n",
      "BSC_Index: \n",
      " Sensitivity of event rankings to DMC transformation: Negligible.  \n",
      " Sensitivity of event numerical meanings to DMC transformation: Very low \n",
      "centre_gravity:\n",
      "BSC_Index: \n",
      " Sensitivity of event rankings to DMC transformation: Negligible.  \n",
      " Sensitivity of event numerical meanings to DMC transformation: Very low \n",
      "m5:\n",
      "BSC_Index: \n",
      " Sensitivity of event rankings to DMC transformation: Negligible.  \n",
      " Sensitivity of event numerical meanings to DMC transformation: Very low \n",
      "3rd_ARR:\n",
      "  Ranking Instability (Kendall's τ): Very low (0.04)\n",
      "  Numerical Instability (Cliff’s Delta): Negligible (0.00)\n",
      "\n",
      "TCI:\n",
      "BSC_Index: \n",
      " Sensitivity of event rankings to DMC transformation: Negligible.  \n",
      " Sensitivity of event numerical meanings to DMC transformation: Very low \n",
      "Huff third:\n",
      "  Ranking Instability (Kendall's τ): Very low (0.05)\n",
      "  Numerical Instability (Cliff’s Delta): Negligible (0.00)\n",
      "\n",
      "T25:\n",
      "BSC_Index: \n",
      " Sensitivity of event rankings to DMC transformation: Negligible.  \n",
      " Sensitivity of event numerical meanings to DMC transformation: Very low \n",
      "Huff quintile:\n",
      "  Ranking Instability (Kendall's τ): Low (0.07)\n",
      "  Numerical Instability (Cliff’s Delta): Negligible (0.00)\n",
      "\n",
      "m4:\n",
      "BSC_Index: \n",
      " Sensitivity of event rankings to DMC transformation: Small.  \n",
      " Sensitivity of event numerical meanings to DMC transformation: Low \n",
      "m3:\n",
      "BSC_Index: \n",
      " Sensitivity of event rankings to DMC transformation: Small.  \n",
      " Sensitivity of event numerical meanings to DMC transformation: Low \n",
      "gini:\n",
      "BSC_Index: \n",
      " Sensitivity of event rankings to DMC transformation: Moderate.  \n",
      " Sensitivity of event numerical meanings to DMC transformation: Low \n",
      "frac_q1_wi_:\n",
      "BSC_Index: \n",
      " Sensitivity of event rankings to DMC transformation: Small.  \n",
      " Sensitivity of event numerical meanings to DMC transformation: Low \n",
      "frac_q4_wi_:\n",
      "BSC_Index: \n",
      " Sensitivity of event rankings to DMC transformation: Negligible.  \n",
      " Sensitivity of event numerical meanings to DMC transformation: Moderate \n",
      "cv:\n",
      "BSC_Index: \n",
      " Sensitivity of event rankings to DMC transformation: Moderate.  \n",
      " Sensitivity of event numerical meanings to DMC transformation: Low \n",
      "frac_q2_wi_:\n",
      "BSC_Index: \n",
      " Sensitivity of event rankings to DMC transformation: Negligible.  \n",
      " Sensitivity of event numerical meanings to DMC transformation: Moderate \n",
      "frac_q3_wi_:\n",
      "BSC_Index: \n",
      " Sensitivity of event rankings to DMC transformation: Negligible.  \n",
      " Sensitivity of event numerical meanings to DMC transformation: Moderate \n",
      "3rd_rcg:\n",
      "  Ranking Instability (Kendall's τ): Moderate (0.20)\n",
      "  Numerical Instability (Cliff’s Delta): Negligible (0.10)\n",
      "\n",
      "event_dry_ratio:\n",
      "BSC_Index: \n",
      " Sensitivity of event rankings to DMC transformation: Moderate.  \n",
      " Sensitivity of event numerical meanings to DMC transformation: Moderate \n",
      "% rain HIZ:\n",
      "BSC_Index: \n",
      " Sensitivity of event rankings to DMC transformation: Small.  \n",
      " Sensitivity of event numerical meanings to DMC transformation: Moderate \n",
      "relative_amp:\n",
      "BSC_Index: \n",
      " Sensitivity of event rankings to DMC transformation: Large.  \n",
      " Sensitivity of event numerical meanings to DMC transformation: Moderate \n",
      "peak_mean_ratio:\n",
      "BSC_Index: \n",
      " Sensitivity of event rankings to DMC transformation: Large.  \n",
      " Sensitivity of event numerical meanings to DMC transformation: Moderate \n",
      "ni:\n",
      "BSC_Index: \n",
      " Sensitivity of event rankings to DMC transformation: Large.  \n",
      " Sensitivity of event numerical meanings to DMC transformation: Moderate \n",
      "peak_mean_ratio_scaled:\n",
      "BSC_Index: \n",
      " Sensitivity of event rankings to DMC transformation: Large.  \n",
      " Sensitivity of event numerical meanings to DMC transformation: Moderate \n",
      "skewp:\n",
      "BSC_Index: \n",
      " Sensitivity of event rankings to DMC transformation: Negligible.  \n",
      " Sensitivity of event numerical meanings to DMC transformation: High \n",
      "lorentz_asymetry:\n",
      "BSC_Index: \n",
      " Sensitivity of event rankings to DMC transformation: Small.  \n",
      " Sensitivity of event numerical meanings to DMC transformation: High \n",
      "% time HIZ:\n",
      "BSC_Index: \n",
      " Sensitivity of event rankings to DMC transformation: Moderate.  \n",
      " Sensitivity of event numerical meanings to DMC transformation: High \n",
      "% time LIZ:\n",
      "BSC_Index: \n",
      " Sensitivity of event rankings to DMC transformation: Moderate.  \n",
      " Sensitivity of event numerical meanings to DMC transformation: High \n",
      "Event Loading:\n",
      "BSC_Index: \n",
      " Sensitivity of event rankings to DMC transformation: Small.  \n",
      " Sensitivity of event numerical meanings to DMC transformation: High \n",
      "skewness:\n",
      "BSC_Index: \n",
      " Sensitivity of event rankings to DMC transformation: Large.  \n",
      " Sensitivity of event numerical meanings to DMC transformation: High \n",
      "intermittency:\n",
      "BSC_Index: \n",
      " Sensitivity of event rankings to DMC transformation: Large.  \n",
      " Sensitivity of event numerical meanings to DMC transformation: Moderate \n",
      "NRMSE_P:\n",
      "BSC_Index: \n",
      " Sensitivity of event rankings to DMC transformation: Large.  \n",
      " Sensitivity of event numerical meanings to DMC transformation: High \n",
      "kurtosis:\n",
      "BSC_Index: \n",
      " Sensitivity of event rankings to DMC transformation: Moderate.  \n",
      " Sensitivity of event numerical meanings to DMC transformation: High \n",
      "m2_wi:\n",
      "BSC_Index: \n",
      " Sensitivity of event rankings to DMC transformation: Large.  \n",
      " Sensitivity of event numerical meanings to DMC transformation: Very high \n",
      "m2:\n",
      "BSC_Index: \n",
      " Sensitivity of event rankings to DMC transformation: Large.  \n",
      " Sensitivity of event numerical meanings to DMC transformation: Very high \n",
      "max_intensity:\n",
      "BSC_Index: \n",
      " Sensitivity of event rankings to DMC transformation: Large.  \n",
      " Sensitivity of event numerical meanings to DMC transformation: High \n",
      "asymm_d:\n",
      "BSC_Index: \n",
      " Sensitivity of event rankings to DMC transformation: Negligible.  \n",
      " Sensitivity of event numerical meanings to DMC transformation: Very high \n",
      "m1_wi:\n",
      "BSC_Index: \n",
      " Sensitivity of event rankings to DMC transformation: Small.  \n",
      " Sensitivity of event numerical meanings to DMC transformation: High \n",
      "m1:\n",
      "BSC_Index: \n",
      " Sensitivity of event rankings to DMC transformation: Small.  \n",
      " Sensitivity of event numerical meanings to DMC transformation: High \n",
      "PCI:\n",
      "BSC_Index: \n",
      " Sensitivity of event rankings to DMC transformation: Large.  \n",
      " Sensitivity of event numerical meanings to DMC transformation: Very high \n",
      "std:\n",
      "BSC_Index: \n",
      " Sensitivity of event rankings to DMC transformation: Large.  \n",
      " Sensitivity of event numerical meanings to DMC transformation: Very high \n",
      "Mean Intensity HIZ:\n",
      "BSC_Index: \n",
      " Sensitivity of event rankings to DMC transformation: Moderate.  \n",
      " Sensitivity of event numerical meanings to DMC transformation: Very high \n",
      "mean_intensity:\n",
      "BSC_Index: \n",
      " Sensitivity of event rankings to DMC transformation: Moderate.  \n",
      " Sensitivity of event numerical meanings to DMC transformation: Very high \n",
      "BSC_Index:\n",
      "  Ranking Instability (Kendall's τ): Very high (1.01)\n",
      "  Numerical Instability (Cliff’s Delta): Medium (0.42)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# for metric in cleaned_sorted_metrics:\n",
    "#     x = all_events_df[metric]\n",
    "#     y = all_events_df[f'{metric}_DMC_10']\n",
    "#     print(f\"{metric}:\")\n",
    "#     if metric in categorical_metric_list:\n",
    "#         instab_rank, meaning_rank = ordinal_ranking_instability(x, y)\n",
    "#         delta, meaning_num = cliffs_delta_fast(x, y)\n",
    "#         print(f\"  Ranking Instability (Kendall's τ): {meaning_rank} ({instab_rank:.2f})\")\n",
    "#         print(f\"  Numerical Instability (Cliff’s Delta): {meaning_num} ({delta:.2f})\\n\")\n",
    "\n",
    "#     else:\n",
    "#         d, meaning = cohens_d(x, y)\n",
    "#         instab, translation = instability_index(x, y)\n",
    "#         print(f\"{base_name}: \\n Sensitivity of event rankings to DMC transformation: {meaning}.  \\n Sensitivity of event numerical meanings to DMC transformation: {translation} \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "de2c1b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, base_name in enumerate(cleaned_sorted_metrics):\n",
    "    \n",
    "    found_pair = False\n",
    "    for suffix in suffixes:\n",
    "        col1 = base_name + suffix\n",
    "        col2 = base_name + '_DMC_10' + suffix\n",
    "        if col1 in scaled_all_events_df.columns and col2 in scaled_all_events_df.columns:\n",
    "            found_pair = True\n",
    "            d, meaning = cohens_d(scaled_all_events_df[col1], scaled_all_events_df[col2])\n",
    "            instab = instability_index(scaled_all_events_df[col1], scaled_all_events_df[col2])\n",
    "#             print(f\"{base_name}:   Cohen's d: {d:.2f}, is {meaning}. Instability is: {instab} \")\n",
    "            print(f\"{base_name}: \\n Sensitivity of event rankings to DMC transformation: {meaning}.  \\n Sensitivity of event numerical meanings to DMC transformation: {translation} \")\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
