{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8026796e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, PowerTransformer\n",
    "from scipy.stats import skew\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c43c7a",
   "metadata": {},
   "source": [
    "### Create one dataframe containing all events for each resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecb41a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_events_5mins =[]\n",
    "# all_events_10mins =[]\n",
    "# all_events_30mins =[]\n",
    "# all_events_60mins =[]\n",
    "\n",
    "# total_events = 0\n",
    "# print(len(os.listdir(\"../DanishRainData_Outputs/5mins/\")))\n",
    "\n",
    "# for num, file in enumerate(np.sort(os.listdir(\"../DanishRainData_Outputs/5mins/\"))):\n",
    "#     if file != \"All_events_571820_precip_minute.csv\":\n",
    "#         # Get 5 mintue data\n",
    "#         df_5m = pd.read_csv(f\"../DanishRainData_Outputs/5mins/{file}\")\n",
    "#         df_5m['event_num']=df_5m.index\n",
    "#         # Get rid of the trailing strings sometimes on the gauge_num\n",
    "#         df_5m['gauge_num'] = df_5m['gauge_num'].apply(\n",
    "#             lambda x: int(x.split('_')[0]) if isinstance(x, str) and x.split('_')[0].isdigit() else x)\n",
    "\n",
    "#         # Get other resolution data\n",
    "#         df_10m = pd.read_csv(f\"../DanishRainData_Outputs/{10}mins/{file}\")\n",
    "#         df_30m = pd.read_csv(f\"../DanishRainData_Outputs/{30}mins/{file}\")\n",
    "#         df_60m = pd.read_csv(f\"../DanishRainData_Outputs/{60}mins/{file}\")\n",
    "\n",
    "#        # Find common event numbers in all four\n",
    "#         common_event_nums = (\n",
    "#             set(df_5m['event_num']) &\n",
    "#             set(df_10m['event_num']) &\n",
    "#             set(df_30m['event_num']) &\n",
    "#             set(df_60m['event_num']))\n",
    "\n",
    "#         # Filter all DataFrames to only include common events\n",
    "#         df_5m = df_5m[df_5m['event_num'].isin(common_event_nums)].reset_index(drop=True)\n",
    "#         df_10m = df_10m[df_10m['event_num'].isin(common_event_nums)].reset_index(drop=True)\n",
    "#         df_30m = df_30m[df_30m['event_num'].isin(common_event_nums)].reset_index(drop=True)\n",
    "#         df_60m = df_60m[df_60m['event_num'].isin(common_event_nums)].reset_index(drop=True)\n",
    "\n",
    "#         total_events = total_events + len(common_event_nums)\n",
    "#         print(f\"{num} : {file}: {len(common_event_nums)} events retained across all resolutions, new total: {total_events}\")    \n",
    "\n",
    "#         # Collect DataFrames into a dictionary for easy iteration\n",
    "#         dfs = {'5m': df_5m,\n",
    "#             '10m': df_10m,\n",
    "#             '30m': df_30m,\n",
    "#             '60m': df_60m}\n",
    "\n",
    "#         # Check the start and end times align\n",
    "#         for i in range(len(df_5m)):\n",
    "#             reference_start = pd.to_datetime(df_5m.iloc[i]['start_time'])\n",
    "#             reference_end = pd.to_datetime(df_5m.iloc[i]['end_time'])\n",
    "#             # print(reference_start,reference_end)\n",
    "#             for res, df in dfs.items():\n",
    "#                 start = pd.to_datetime(df.iloc[i]['start_time'])\n",
    "#                 end = pd.to_datetime(df.iloc[i]['end_time'])\n",
    "#                 #print(res, start, end)\n",
    "\n",
    "#                 delta_start = abs(start - reference_start)\n",
    "#                 delta_end = abs(end - reference_end)\n",
    "\n",
    "#                 if delta_start > timedelta(minutes=60):\n",
    "#                     print(f\"⚠️ START mismatch in {res} at index {i}\")\n",
    "#                     print(f\"   {res} start: {start}, 5m start: {reference_start}, Δ: {delta_start}\")\n",
    "\n",
    "#                 if delta_end > timedelta(minutes=60):\n",
    "#                     print(f\"⚠️ END mismatch in {res} at index {i}\")\n",
    "#                     print(f\"   {res} end: {end}, 5m end: {reference_end}, Δ: {delta_end}\")\n",
    "\n",
    "#         all_events_5mins.append(df_5m)\n",
    "#         all_events_10mins.append(df_10m)             \n",
    "#         all_events_30mins.append(df_30m)\n",
    "#         all_events_60mins.append(df_60m)            \n",
    "\n",
    "# all_events_df_5mins = pd.concat(all_events_5mins)   \n",
    "# all_events_df_10mins = pd.concat(all_events_10mins)   \n",
    "# all_events_df_30mins = pd.concat(all_events_30mins)\n",
    "# all_events_df_60mins = pd.concat(all_events_60mins)   \n",
    "\n",
    "# for df in [all_events_df_5mins, all_events_df_10mins, all_events_df_30mins, all_events_df_60mins]:\n",
    "#     df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "# print(len(all_events_df_5mins), len(all_events_df_10mins), len(all_events_df_30mins), len(all_events_df_60mins))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c33e4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_events_df_5mins.to_csv('Data/all_events_df_5mins.csv', index=False)\n",
    "# all_events_df_10mins.to_csv('Data/all_events_df_10mins.csv', index=False)\n",
    "# all_events_df_30mins.to_csv('Data/all_events_df_30mins.csv', index=False)\n",
    "# all_events_df_60mins.to_csv('Data/all_events_df_60mins.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc47097",
   "metadata": {},
   "source": [
    "### Scale and transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96107324",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_events_df_5mins = pd.read_csv('Data/all_events_df_5mins.csv')\n",
    "all_events_df_10mins = pd.read_csv('Data/all_events_df_10mins.csv')\n",
    "all_events_df_30mins = pd.read_csv('Data/all_events_df_30mins.csv')\n",
    "all_events_df_60mins = pd.read_csv('Data/all_events_df_60mins.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfaa9b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_events_df_5mins = all_events_df_5mins.rename(columns={'lorentz_asymetry': 'lorenz_asymmetry'})\n",
    "# all_events_df_10mins = all_events_df_10mins.rename(columns={'lorentz_asymetry': 'lorenz_asymmetry'})\n",
    "# all_events_df_30mins = all_events_df_30mins.rename(columns={'lorentz_asymetry': 'lorenz_asymmetry'})\n",
    "# all_events_df_60mins = all_events_df_60mins.rename(columns={'lorentz_asymetry': 'lorenz_asymmetry'})\n",
    "\n",
    "# all_events_df_5mins = all_events_df_5mins.rename(columns={'lorentz_asymetry_DMC_10': 'lorenz_asymmetry_DMC_10'})\n",
    "# all_events_df_10mins = all_events_df_10mins.rename(columns={'lorentz_asymetry_DMC_10': 'lorenz_asymmetry_DMC_10'})\n",
    "# all_events_df_30mins = all_events_df_30mins.rename(columns={'lorentz_asymetry_DMC_10': 'lorenz_asymmetry_DMC_10'})\n",
    "# all_events_df_60mins = all_events_df_60mins.rename(columns={'lorentz_asymetry_DMC_10': 'lorenz_asymmetry_DMC_10'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27932e6",
   "metadata": {},
   "source": [
    "### Delete columns we dont want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d25baf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "106bed76",
   "metadata": {},
   "source": [
    "### Remove rows with unrealistically high precip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f219c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "397157 397157 397157 397157\n"
     ]
    }
   ],
   "source": [
    "bad_indices = all_events_df_5mins.sort_values(by=\"total_precip\", ascending=False)[:15].index\n",
    "\n",
    "# Drop rows at these indices from all dataframes\n",
    "all_events_df_5mins = all_events_df_5mins.drop(index=bad_indices)\n",
    "all_events_df_10mins = all_events_df_10mins.drop(index=bad_indices)\n",
    "all_events_df_30mins = all_events_df_30mins.drop(index=bad_indices)\n",
    "all_events_df_60mins = all_events_df_60mins.drop(index=bad_indices)\n",
    "\n",
    "for df in [all_events_df_5mins, all_events_df_10mins, all_events_df_30mins, all_events_df_60mins]:\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "print(len(all_events_df_5mins), len(all_events_df_10mins), len(all_events_df_30mins), len(all_events_df_60mins))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec642691",
   "metadata": {},
   "source": [
    "### Remove very small events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0adac391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "397157\n",
      "240579\n",
      "397157\n",
      "240590\n",
      "397157\n",
      "240609\n",
      "397157\n",
      "240588\n"
     ]
    }
   ],
   "source": [
    "print(len(all_events_df_5mins))\n",
    "all_events_df_5mins = all_events_df_5mins[all_events_df_5mins['total_precip']>4].copy()\n",
    "print(len(all_events_df_5mins))\n",
    "\n",
    "print(len(all_events_df_10mins))\n",
    "all_events_df_10mins = all_events_df_10mins[all_events_df_10mins['total_precip']>4].copy()\n",
    "print(len(all_events_df_10mins))\n",
    "\n",
    "print(len(all_events_df_30mins))\n",
    "all_events_df_30mins = all_events_df_30mins[all_events_df_30mins['total_precip']>4].copy()\n",
    "print(len(all_events_df_30mins))\n",
    "\n",
    "print(len(all_events_df_60mins))\n",
    "all_events_df_60mins = all_events_df_60mins[all_events_df_60mins['total_precip']>4].copy()\n",
    "print(len(all_events_df_60mins))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576f27e1",
   "metadata": {},
   "source": [
    "# Create version for all resolutions, just raw\n",
    "### Specify just raw columns to keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96249375",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_cols = []\n",
    "for col in all_events_df_5mins.columns:\n",
    "    if not col.endswith('_DMC_10') and not col.endswith('dblnorm'):\n",
    "        raw_cols.append(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888a84d2",
   "metadata": {},
   "source": [
    "### Also remove 'frac_q1', 'frac_q2', 'frac_q3', 'frac_q4'\n",
    "Because these bring lots of nans (with current calculation method)  \n",
    "ni is the same as peak mean ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ba3a73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_del = ['frac_q1', 'frac_q2', 'frac_q3', 'frac_q4', 'BSC', 'ni', 'm1', 'm2', 'm3', 'm4', 'm5']\n",
    "more_cols_to_del = ['gauge_num',  'start_time', 'end_time', 'min_intensity', 'heaviest_half']\n",
    "#                     'duration' ,'total_precip',  'event_num',\n",
    "#                     'peak_mean_ratio_scaled',\n",
    "                   \n",
    "raw_cols = [x for x in raw_cols if x not in cols_to_del]  \n",
    "raw_cols = [x for x in raw_cols if x not in more_cols_to_del]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2067369",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_events_df_5mins_raw = all_events_df_5mins[raw_cols].copy()\n",
    "all_events_df_10mins_raw = all_events_df_10mins[raw_cols].copy()\n",
    "all_events_df_30mins_raw = all_events_df_30mins[raw_cols].copy()\n",
    "all_events_df_60mins_raw = all_events_df_60mins[raw_cols].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f54d4472",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_events_df_60mins_raw[\"I30\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67ea1c7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240579 240590 240609 240588\n",
      "Columns with at least one NaN value:\n",
      "[]\n",
      "0\n",
      "Columns with at least one NaN value:\n",
      "[]\n",
      "0\n",
      "Columns with at least one NaN value:\n",
      "[]\n",
      "0\n",
      "Columns with at least one NaN value:\n",
      "['kurtosis', 'asymm_d']\n",
      "7632\n",
      "232947 232958 232977 232956\n"
     ]
    }
   ],
   "source": [
    "print(len(all_events_df_5mins_raw), len(all_events_df_10mins_raw), len(all_events_df_30mins_raw), len(all_events_df_60mins_raw))\n",
    "\n",
    "bad_indices_all = []\n",
    "for all_events_df in [all_events_df_5mins_raw, all_events_df_10mins_raw, all_events_df_30mins_raw, all_events_df_60mins_raw]:\n",
    "    columns_with_nan = all_events_df.columns[all_events_df.isnull().any()].tolist()\n",
    "    print(\"Columns with at least one NaN value:\")\n",
    "    print(columns_with_nan)\n",
    "    # #Find indices of rows with any NaNs in the 60-minute data\n",
    "    bad_indices = all_events_df[all_events_df.isnull().any(axis=1)].index\n",
    "    bad_indices_lst = bad_indices.tolist()\n",
    "    print(len(bad_indices_lst))\n",
    "    if len(bad_indices) >0:\n",
    "        bad_indices_all.extend(bad_indices_lst)\n",
    "    \n",
    "# Drop rows at these indices from all dataframes\n",
    "all_events_df_5mins_raw = all_events_df_5mins_raw.drop(index=bad_indices_all)\n",
    "all_events_df_10mins_raw = all_events_df_10mins_raw.drop(index=bad_indices_all)\n",
    "all_events_df_30mins_raw = all_events_df_30mins_raw.drop(index=bad_indices_all)\n",
    "all_events_df_60mins_raw = all_events_df_60mins_raw.drop(index=bad_indices_all)   \n",
    "\n",
    "print(len(all_events_df_5mins_raw), len(all_events_df_10mins_raw), len(all_events_df_30mins_raw), len(all_events_df_60mins_raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634e6a93",
   "metadata": {},
   "source": [
    "### Join all into one dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5845c4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_columns =all_events_df_5mins_raw.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4113f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_events_df_5mins_raw = all_events_df_5mins_raw.add_suffix('_5m')\n",
    "all_events_df_10mins_raw = all_events_df_10mins_raw.add_suffix('_10m')\n",
    "all_events_df_30mins_raw = all_events_df_30mins_raw.add_suffix('_30m')\n",
    "all_events_df_60mins_raw = all_events_df_60mins_raw.add_suffix('_60m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b192d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_events_all_res = pd.concat([all_events_df_5mins_raw, all_events_df_10mins_raw,\n",
    "                               all_events_df_30mins_raw, all_events_df_60mins_raw], axis=1)\n",
    "# Step 2: Define the event number columns from each resolution\n",
    "event_cols = ['event_num_5m', 'event_num_10m', 'event_num_30m', 'event_num_60m']\n",
    "\n",
    "# Step 3: Drop rows with any missing event numbers\n",
    "all_events_all_res = all_events_all_res.dropna(subset=event_cols)\n",
    "\n",
    "# Step 4: Keep only rows where all event numbers are equal (same event matched across resolutions)\n",
    "all_events_all_res = all_events_all_res[all_events_all_res[event_cols].nunique(axis=1) == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a77af82",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_events_all_res.to_csv(\"Data/NotScaled_AllRes.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fba4e4f",
   "metadata": {},
   "source": [
    "# Create version for 5 minutes, DMC and raw\n",
    "### Specify columns to keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5ca8bb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_del = ['frac_q1', 'frac_q2', 'frac_q3', 'frac_q4', 'BSC', 'ni', 'T50', 'm1', 'm2', 'm3', 'm4', 'm5', 'BSC_Index']\n",
    "more_cols_to_del = ['gauge_num', 'start_time', 'end_time', 'duration', 'total_precip', 'event_num', 'heaviest_half']\n",
    "all_to_del = cols_to_del + more_cols_to_del\n",
    "all_columns = df.columns.tolist()\n",
    "cols_to_keep = [col for col in all_columns if col not in all_to_del]\n",
    "\n",
    "# Remove any column that matches exactly or starts with one of the names (e.g., has suffixes like _DMC_10)\n",
    "cols_to_keep = [col for col in cols_to_keep if col not in all_to_del]\n",
    "\n",
    "# Then filter the DataFrame\n",
    "all_events_df_5mins_filtered = all_events_df_5mins[cols_to_keep].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8171850f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_events_df_5mins_filtered['mean_intensity_DMC_10'] = all_events_df_5mins_filtered['mean_intensity_DMC_10']/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d75224d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_events_df_5mins_filtered.to_csv(\"Data/NotScaled_RawVsDMC.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
