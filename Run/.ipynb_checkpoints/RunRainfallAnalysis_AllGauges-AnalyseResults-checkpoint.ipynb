{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff30609b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from scipy.interpolate import interp1d\n",
    "import seaborn as sns\n",
    "import os\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import pickle\n",
    "import datetime \n",
    "import warnings\n",
    "import sys\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "from ClassFunctions import precip_time_series, rainfall_analysis\n",
    "sys.path.insert(0, '../')\n",
    "from PlottingFunctions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98f1e8c",
   "metadata": {},
   "source": [
    "## Create object containing rainfall events for one gauge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "053c7a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from scipy import stats\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.stats import skew, kurtosis\n",
    "import datetime\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "temp_res = 5\n",
    "\n",
    "class precip_time_series:\n",
    "    def __init__(self, data_path):\n",
    "        print(\"Making the class so I am\")\n",
    "        self.data,self.statid = self.read_raw_data_as_pandas_df(data_path)\n",
    "        \n",
    "        self.padded = False\n",
    "\n",
    "        self.events = None # first and last time step\n",
    "         \n",
    "        self.raw_events = None # the actual event\n",
    "        \n",
    "        self.normalised_events = None # event with intensity normalised\n",
    "        \n",
    "        self.double_normalised_events = None # event with both time and intensity normalised\n",
    "        \n",
    "        self.interpolated_events = None # \n",
    "        \n",
    "        self.DMCs = None\n",
    "        \n",
    "        self.DMCs_100 = None\n",
    "        \n",
    "    def read_raw_data_as_pandas_df_rasmus(self,raw_data_file_path):\n",
    "        # Read file with timestamp as index\n",
    "        precip = pd.read_excel(raw_data_file_path,index_col=1)\n",
    "\n",
    "        # Timestamps str -> datetime\n",
    "        precip.index = pd.to_datetime(precip.index)\n",
    "\n",
    "        # Save ID of station\n",
    "        station_id = str(precip.station.iloc[0])\n",
    "\n",
    "        # Remove column with station ID\n",
    "        precip = precip.drop(\"station\",axis=1)\n",
    "        \n",
    "        return precip,station_id\n",
    " \n",
    "    def read_raw_data_as_pandas_df(self, raw_data_file_path):\n",
    "\n",
    "        # Read file with timestamp as index\n",
    "        precip = pd.read_csv(raw_data_file_path, encoding=\"ISO-8859-1\",index_col=0)\n",
    "        precip.rename(columns={'precip_past1min':'precipitation (mm/hr)'},inplace=True)\n",
    "\n",
    "        precip = precip[precip['precipitation (mm/hr)'] !=-9999]\n",
    "        precip = precip[precip['precipitation (mm/hr)'] !=-999999.0]\n",
    "        precip.reset_index(inplace=True)\n",
    "        precip['timeobs'] = precip['timeobs'].apply(lambda x: datetime.datetime.fromtimestamp(x))\n",
    "        precip.set_index('timeobs', inplace=True)        \n",
    "        \n",
    "        # Save ID of station\n",
    "        filename = raw_data_file_path.split('/')[-1]\n",
    "        station_id = filename.split('_')[0]\n",
    "        print(station_id)  # Output: 615600\n",
    "        \n",
    "        # Get rid of duplicates\n",
    "        precip  = self.resolve_duplicates(precip)\n",
    "\n",
    "        # Fill in rows which should have a 0 value\n",
    "        precip = self.fill_in_missing_vals(precip)\n",
    "\n",
    "        # Run QC\n",
    "        qc = pd.read_csv(f\"/nfs/a319/gy17m2a/PhD/datadir/station_exclusion_periods_from_climadb.csv\")\n",
    "        qc.rename(columns = {'the_date':'start_time', 'hour':'end_time'}, inplace=True)\n",
    "        qc_this_station = qc[qc['statid'] == int(station_id)]\n",
    "        precip = self.run_quality_control(precip, qc_this_station)\n",
    "        precip.loc[precip['QC_fail'], 'precipitation (mm/hr)'] = np.nan\n",
    "        # Timestamps str -> datetime\n",
    "        precip.index = pd.to_datetime(precip.index)\n",
    "        del precip['QC_fail']\n",
    "        if len(precip[precip[\"precipitation (mm/hr)\"]<0])>0 :\n",
    "            print(\"Values less than 0\")\n",
    "\n",
    "        return precip,station_id   \n",
    "    \n",
    "    def resolve_duplicates(self, df):\n",
    "        # This list will store flags for indices where duplicate rows differ.\n",
    "        duplicate_flags = []\n",
    "\n",
    "        # Get all rows with duplicated indices (all occurrences)\n",
    "        dup_df = df[df.index.duplicated(keep=False)]\n",
    "\n",
    "        # Group by the index\n",
    "        grouped = dup_df.groupby(dup_df.index)\n",
    "\n",
    "        # Iterate over each group (each duplicated index)\n",
    "        for idx, group in grouped:\n",
    "            if len(group) > 1:\n",
    "                # Check if all rows are identical by comparing each to the first row.\n",
    "                # group.eq(group.iloc[0]) returns a DataFrame of booleans.\n",
    "                if group.eq(group.iloc[0]).all().all():\n",
    "                    # They are identical: flag it and keep only one row.\n",
    "                    # print(f\"Index {idx} has duplicate rows that are identical. Keeping one.\")\n",
    "                    #duplicate_flags.append((idx, \"identical\"))\n",
    "                    pass\n",
    "                else:\n",
    "                    # They differ: flag this index.\n",
    "                    # print(f\"Warning: Index {idx} has duplicate rows that differ!\")\n",
    "                    duplicate_flags.append((idx, \"differ\"))\n",
    "\n",
    "        # Remove duplicate rows, keeping only the first occurrence for each index.\n",
    "        print(f\"duplicate flags length: {len(duplicate_flags)}\")\n",
    "        df_cleaned = df[~df.index.duplicated(keep='first')]\n",
    "        return df_cleaned    \n",
    "    \n",
    "    def fill_in_missing_vals(self, df):\n",
    "        # Get the full range of datetime values (1-minute intervals)\n",
    "        start_time = df.index.min()\n",
    "        end_time = df.index.max()\n",
    "        full_range = pd.date_range(start=start_time, end=end_time, freq='1T')\n",
    "        \n",
    "        df.index = pd.to_datetime(df.index).floor('T')\n",
    "        # Reindex the DataFrame to the complete range.\n",
    "        # Missing rows will have NaN in the original columns.\n",
    "        df_reindexed = df.reindex(full_range)\n",
    "        df_reindexed = df_reindexed.fillna(0)\n",
    "\n",
    "        # Add a new column 'QC_fail'\n",
    "        # For rows that were missing in the original index, flag them as True.\n",
    "        # df_reindexed['QC_fail'] = ~df_reindexed.index.isin(df.index)\n",
    "        # df_reindexed = df.reindex(full_range).fillna(0)\n",
    "\n",
    "        return df_reindexed    \n",
    "    \n",
    "    def run_quality_control(self, df, qc_this_station):\n",
    "        df_filtered=df.copy()\n",
    "        df_filtered['QC_fail'] = False\n",
    "        \n",
    "        for i in range(len(qc_this_station)):\n",
    "            # Get the start and end time of that missing chunk\n",
    "            start_time =  pd.Timestamp(qc_this_station.iloc[i]['start_time'])\n",
    "            end_time = qc_this_station.iloc[i]['end_time']\n",
    "            # Create an extra start time variable, to ensure the hour leading up to the first date is included\n",
    "            extra_start_time = start_time - pd.Timedelta(hours=1)\n",
    "\n",
    "            # Create a list of datetime values at 1-minute intervals\n",
    "            time_list = pd.date_range(start=extra_start_time, end=end_time, freq=\"T\").to_list()\n",
    "            # Ensure time_list is a DatetimeIndex for efficient lookup\n",
    "            time_list = pd.to_datetime(time_list)  # Convert to DatetimeIndex if needed\n",
    "\n",
    "            # Remove bad values\n",
    "            df_filtered.loc[df_filtered.index.isin(time_list), 'QC_fail'] = True   \n",
    "            \n",
    "        return df_filtered\n",
    "    \n",
    "    def pad_and_resample(self, temp_res ,pad_value = 0):\n",
    "        # Resample the data to the specified frequency and pad missing values with pad_value\n",
    "        freq = f'{temp_res}min'\n",
    "        self.data = self.data.resample(freq).sum().fillna(pad_value)\n",
    "        self.data *= 60 / int(temp_res)\n",
    "        self.padded = True\n",
    "        \n",
    "    def return_specific_event(self,event_idx):\n",
    "\n",
    "        # Size of timesteps\n",
    "        time_delta = self.data.index[1] - self.data.index[0]\n",
    "        time_delta_minutes = time_delta.seconds / 60\n",
    "\n",
    "        # Extract event data\n",
    "        event = self.data.loc[self.events[event_idx][0]:self.events[event_idx][1]]\n",
    "\n",
    "        return event  \n",
    "    \n",
    "    def return_specific_double_normalised_event(self,event_idx):\n",
    "\n",
    "        # Size of timesteps\n",
    "        time_delta = self.data.index[1] - self.data.index[0]\n",
    "        time_delta_minutes = time_delta.seconds / 60\n",
    "        \n",
    "        # Extract event data\n",
    "        event = self.double_normalised_events[event_idx]\n",
    "\n",
    "        return event    \n",
    "\n",
    "    def return_specific_normalised_event(self,event_idx):\n",
    "\n",
    "        # Size of timesteps\n",
    "        time_delta = self.data.index[1] - self.data.index[0]\n",
    "        time_delta_minutes = time_delta.seconds / 60\n",
    "        \n",
    "        # Extract event data\n",
    "        event = self.normalised_events[event_idx]\n",
    "\n",
    "        return event       \n",
    "    \n",
    "    def return_specific_interpolated_event(self,event_idx):\n",
    "        \n",
    "        # Extract event data\n",
    "        event = self.interpolated_events[event_idx]\n",
    "\n",
    "        return event     \n",
    "        \n",
    "    def get_events(self,threshold,min_duration = 30, min_precip = 1):\n",
    "        \n",
    "        if not self.padded:\n",
    "            self.pad_and_resample()\n",
    "\n",
    "        self.init_events(threshold)\n",
    "        self.filter_events_by_length(min_duration)\n",
    "        self.filter_events_by_amount(min_precip)\n",
    "               \n",
    "\n",
    "    def init_events(self, threshold):\n",
    "        precip = self.data\n",
    "\n",
    "        ########################################\n",
    "        ##### Prep: Calculate time step and rolling window\n",
    "        # This calculates how many data points are in your threshold.\n",
    "        # If your data is at 5-min intervals, then '11h' = 132 steps.\n",
    "        ########################################\n",
    "\n",
    "        time_delta = precip.index[1] - precip.index[0]\n",
    "        threshold_minutes = pd.to_timedelta(threshold).total_seconds() / 60\n",
    "        window_size = int(threshold_minutes // (time_delta.total_seconds() / 60))\n",
    "\n",
    "        ########################################\n",
    "        #####  2. Rolling sum and rolling count\n",
    "        # Precip_sum converts the column to one showing the total precip in the last 11h\n",
    "        # Valid count makes a column showing the number of valid precip vals in the last 11h (i.e. not NAN)\n",
    "        ########################################    \n",
    "        precip_sum = precip.rolling(window_size).sum(min_periods=1)\n",
    "        precip_sum['precipitation (mm/hr)'] = precip_sum['precipitation (mm/hr)'].apply(lambda x: 0 if abs(x) < 1e-10 else x)\n",
    "        valid_count = precip.rolling(window_size).count()\n",
    "\n",
    "        ########################################\n",
    "        #####3. Identify dry periods\n",
    "        # Dry if rolling sum is 0 and full data coverage\n",
    "        ########################################\n",
    "        is_dry = (precip_sum == 0) & (valid_count == window_size)\n",
    "\n",
    "        ########################################\n",
    "        #####4. Loop through to find event boundaries\n",
    "        # Uses a state machine approach to find paired start/end times\n",
    "        ########################################\n",
    "        events = []\n",
    "        in_event = False\n",
    "        current_start = None\n",
    "\n",
    "        for i in range(1, len(precip)):\n",
    "            prev_dry = is_dry.iloc[i - 1, 0]\n",
    "            curr_dry = is_dry.iloc[i, 0]\n",
    "\n",
    "            # Start of a rain event (dry -> wet)\n",
    "            if not curr_dry and prev_dry and not in_event:\n",
    "                current_start = precip.index[i]\n",
    "                in_event = True\n",
    "\n",
    "            # End of a rain event (wet -> dry)\n",
    "            elif curr_dry and not prev_dry and in_event:\n",
    "                current_end = precip.index[i] - pd.to_timedelta(threshold)\n",
    "                events.append((current_start, current_end))\n",
    "                in_event = False\n",
    "\n",
    "        ########################################\n",
    "        #####5. Close last event if still raining at end\n",
    "        ########################################\n",
    "        if in_event:\n",
    "            for i in reversed(range(len(precip))):\n",
    "                if precip.iloc[i, 0] != 0:\n",
    "                    events.append((current_start, precip.index[i]))\n",
    "                    break\n",
    "\n",
    "        ########################################        \n",
    "        #####6. Store events in class attribute\n",
    "        ########################################\n",
    "        self.events = events\n",
    "\n",
    "\n",
    "    def filter_events_by_length(self,min_duration):\n",
    "        \n",
    "        # Remove events with duration under min duration\n",
    "        filtered_events = [event for event in self.events if event[1]-event[0]>=pd.Timedelta(minutes=min_duration)]\n",
    "\n",
    "        # Update events\n",
    "        self.events = filtered_events\n",
    "    \n",
    "    def filter_events_by_amount(self,min_precip):\n",
    "        \n",
    "        # Remove events with total precip under minimum\n",
    "        filtered_events = [event for event in self.events if self.data.loc[event[0]:event[1]].sum().values[0]>=min_precip]\n",
    "        \n",
    "        # update events\n",
    "        self.events = filtered_events\n",
    "        \n",
    "    def create_raw_events(self, threshold):\n",
    "\n",
    "        # Make sure events have been computed\n",
    "        if self.events == None:\n",
    "            self.get_events(threshold=threshold)\n",
    "        \n",
    "        # Make list of nparrays containing the values of the normalised curve\n",
    "        raw_events = [self.data.loc[event[0]:event[1]] for event in self.events]\n",
    "\n",
    "        # Assign to global value\n",
    "        self.raw_events = raw_events        \n",
    "\n",
    "    def create_double_normalised_events(self, threshold):\n",
    "        # Make sure events have been computed\n",
    "        if self.events is None:\n",
    "            self.get_events(threshold=threshold)\n",
    "\n",
    "        # Make list of nparrays containing the values of the double-normalized curve\n",
    "        double_normalised_events = [self.get_double_normalised_event(self.data.loc[event[0]:event[1]].values) for event in self.events]\n",
    "\n",
    "        # Assign to global value\n",
    "        self.double_normalised_events = double_normalised_events\n",
    "\n",
    "    def create_normalised_events(self, threshold):\n",
    "\n",
    "        # Make sure events have been computed\n",
    "        if self.events == None:\n",
    "            self.get_events(threshold=threshold)\n",
    "        \n",
    "        # Make list of nparrays containing the values of the normalised curve\n",
    "        normalised_events = [self.get_normalised_event(self.data.loc[event[0]:event[1]].values) for event in self.events]\n",
    "\n",
    "        # Assign to global value\n",
    "        self.normalised_events = normalised_events\n",
    "        \n",
    "    def create_incremental_event(self, cumulative_rainfall_df):\n",
    "        if cumulative_rainfall_df is None:\n",
    "            return None\n",
    "\n",
    "        # Calculate incremental rainfall by differencing, preserving index\n",
    "        incremental_rainfall = np.diff(cumulative_rainfall_df, prepend=0)\n",
    "        time_normalized = np.linspace(0, 1,len(incremental_rainfall))\n",
    "        \n",
    "        df =pd.DataFrame({'DMC': incremental_rainfall}, index=time_normalized)\n",
    "        return df    \n",
    "\n",
    "    def create_DMCs (self, threshold):\n",
    "        # Make sure events have been computed\n",
    "        if self.events is None:\n",
    "            self.get_events(threshold=threshold)\n",
    "\n",
    "        # Make list of nparrays containing the values of the double-normalized curve\n",
    "        DMCs_10 = [self.get_interpolated_event(event, 10) for event in self.double_normalised_events]\n",
    "        DMCS_10_incremental = [self.create_incremental_event(event['DMC']) for event in DMCs_10]\n",
    "        self.DMCs = DMCS_10_incremental  \n",
    "        \n",
    "        DMCs_100 = [self.get_interpolated_event(event, 100) for event in self.double_normalised_events]\n",
    "        DMCS_100_incremental = [self.create_incremental_event(event['DMC']) for event in DMCs_100]\n",
    "        self.DMCs_100 = DMCS_100_incremental         \n",
    "    \n",
    "    def get_normalised_event(self,series):\n",
    "    \n",
    "        # Calculate cumulative rainfall\n",
    "        cumulative_rainfall = np.cumsum(series)\n",
    "        # cumulative_rainfall = np.append([0],cumulative_rainfall)\n",
    "\n",
    "        # normalize\n",
    "        normalized_cumulative_rainfall = cumulative_rainfall/cumulative_rainfall[-1]\n",
    "        time_normalized = np.linspace(0, 1,len(normalized_cumulative_rainfall))\n",
    "        normalised_df = pd.DataFrame({'normalised_rainfall':normalized_cumulative_rainfall}, index=time_normalized)\n",
    "        \n",
    "        return normalised_df\n",
    "\n",
    "    def get_double_normalised_event(self, series):\n",
    "        # Calculate cumulative rainfall\n",
    "        cumulative_rainfall = np.cumsum(series)\n",
    "        # cumulative_rainfall = np.append([0], cumulative_rainfall)\n",
    "\n",
    "        # Normalize cumulative rainfall\n",
    "        normalized_cumulative_rainfall = cumulative_rainfall / cumulative_rainfall[-1]\n",
    "\n",
    "        # Normalize the time axis (assuming time is at regular intervals)\n",
    "        time_normalized = np.linspace(0, 1,len(normalized_cumulative_rainfall))\n",
    "        \n",
    "        double_normalised_df = pd.DataFrame({'normalised_rainfall':normalized_cumulative_rainfall}, index=time_normalized)\n",
    "\n",
    "        return double_normalised_df\n",
    "\n",
    "    def get_interpolated_event(self, normalized_cumulative_rainfall, n):\n",
    "        normalized_cumulative_rainfall = normalized_cumulative_rainfall['normalised_rainfall']\n",
    "        # Define target points for bin_number bins\n",
    "        target_points = np.linspace(0, 1, n)\n",
    "\n",
    "        # Create interpolation function based on existing data points\n",
    "        rainfall_times = np.array(range(0, len(normalized_cumulative_rainfall)))\n",
    "\n",
    "        # Normalize time from 0 to 1\n",
    "        normalized_time = (rainfall_times - rainfall_times[0]) / (rainfall_times[-1] - rainfall_times[0])\n",
    "        interpolation_func = interp1d(normalized_time, normalized_cumulative_rainfall, kind='linear', fill_value=\"extrapolate\")\n",
    "\n",
    "        # Interpolate values at target points\n",
    "        interpolated_values = interpolation_func(target_points)\n",
    "        interpolated_values = np.round(interpolated_values, 6)\n",
    "        interpolated_values_df =pd.DataFrame({'DMC':interpolated_values}, index=target_points)\n",
    "        return interpolated_values_df        \n",
    "    \n",
    "    def interpolate_rainfall(self,dim_less_curve,bin):\n",
    "\n",
    "        # Define target points for splits\n",
    "        target_points = np.linspace(0, 1, bin+1)\n",
    "\n",
    "        # normalized time\n",
    "        normalized_time = np.linspace(0, 1,len(dim_less_curve))\n",
    "\n",
    "        # Create interpolation function based on existing data points\n",
    "        interpolation_func = interp1d(normalized_time, dim_less_curve, kind='linear', fill_value=\"extrapolate\")\n",
    "\n",
    "        # Interpolate values at target points\n",
    "        interpolated_values = interpolation_func(target_points)\n",
    "\n",
    "        return interpolated_values    \n",
    "   \n",
    "    def plot_all_events(self):\n",
    "        plt.figure(figsize=(20,10))\n",
    "        plt.plot(self.data.index,self.data.values)\n",
    "        for i,dates in enumerate(self.events):\n",
    "            plt.vlines(dates[0],colors=\"green\",linestyles='--',ymin=0,ymax=3)\n",
    "            plt.vlines(dates[1],colors=\"red\",linestyles='--',ymin=0,ymax=3)\n",
    "        plt.legend([\"Precipitation\",\"Event start\",\"Event end\"])\n",
    "        plt.ylabel(\"[mm]\")\n",
    "        plt.title(\"Padded precip data, with events\")\n",
    "\n",
    "    def plot_specific_event(self,event_idx):\n",
    "\n",
    "        # Size of timesteps\n",
    "        time_delta = self.data.index[1]-self.data.index[0]\n",
    "        time_delta_minuts = time_delta.seconds/60\n",
    "\n",
    "        plt.figure()\n",
    "        event = (self.data.loc[self.events[event_idx][0]:self.events[event_idx][1]])\n",
    "\n",
    "        # plot were right edge align w timestamp\n",
    "        plt.bar(event.index-time_delta,event.values[:,0],width=pd.Timedelta(minutes=time_delta_minuts),align=\"edge\")\n",
    "\n",
    "        plt.legend([\"Precipitation\"])\n",
    "        plt.title(f\"Event {event_idx}\")\n",
    "\n",
    "    def plot_specific_event_w_hist(self,event_idx):\n",
    "\n",
    "        # Size of timesteps\n",
    "        time_delta = self.data.index[1] - self.data.index[0]\n",
    "        time_delta_minutes = time_delta.seconds / 60\n",
    "\n",
    "        # Extract event data\n",
    "        event = self.data.loc[self.events[event_idx][0]:self.events[event_idx][1]]\n",
    "\n",
    "        # Create a figure with two subplots (1 row, 2 columns)\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "        # **Plot time series (left subplot)**\n",
    "        axes[0].bar(event.index - time_delta, event.values[:, 0], \n",
    "                    width=pd.Timedelta(minutes=time_delta_minutes), align=\"edge\")\n",
    "        axes[0].set_title(f\"Event {event_idx}\")\n",
    "        axes[0].set_ylabel(\"Precipitation (mm)\")\n",
    "        axes[0].set_xlabel(\"Time\")\n",
    "        axes[0].legend([\"Precipitation\"])\n",
    "\n",
    "        # **Plot histogram (right subplot)**\n",
    "        axes[1].hist(event.values[:, 0], bins=10, edgecolor='black', alpha=0.7)\n",
    "        axes[1].set_title(\"Precipitation Histogram\")\n",
    "        axes[1].set_xlabel(\"Precipitation (mm)\")\n",
    "        axes[1].set_ylabel(\"Frequency\")\n",
    "\n",
    "        # Adjust layout for clarity\n",
    "        plt.tight_layout()\n",
    "\n",
    "    def plot_specific_normalised_events(self,event_idxs):\n",
    "        plt.figure()\n",
    "        for idx in event_idxs:\n",
    "            x_values = np.linspace(0,1,len(self.normalised_events[idx]))\n",
    "            plt.plot(x_values,self.normalised_events[idx],label = f\"Event: {idx+1}\")\n",
    "        plt.legend()\n",
    "        plt.title(\"normalised events\")\n",
    "        \n",
    "    def plot_specific_normalised_event(self,event_idx):\n",
    "        plt.figure()\n",
    "        x_values = np.linspace(0,1,len(self.normalised_events[event_idx]))\n",
    "        plt.plot(x_values,self.normalised_events[event_idx],label = f\"Event: {event_idx+1}\")\n",
    "        plt.legend()\n",
    "        plt.title(\"normalised events\")        \n",
    "\n",
    "class rainfall_analysis:\n",
    "    def __init__(self,threshold, ts: precip_time_series):\n",
    "        self.ts = ts\n",
    "        self.metrics = {} \n",
    "        \n",
    "        # Prepere ts for analysis\n",
    "        if not self.ts.padded:\n",
    "            ts.pad_and_resample()\n",
    "\n",
    "        if ts.events == None:\n",
    "            ts.get_events(threshold=threshold)\n",
    "            \n",
    "        if ts.raw_events == None:\n",
    "            ts.create_raw_events(threshold)  \n",
    "            \n",
    "        if ts.normalised_events == None:\n",
    "            ts.create_normalised_events(threshold)\n",
    "            \n",
    "        if ts.double_normalised_events == None:\n",
    "            ts.create_double_normalised_events(threshold)      \n",
    "            \n",
    "        if ts.DMCs == None:\n",
    "            ts.create_DMCs(threshold)   \n",
    "    \n",
    "    def interpolate_rainfall(self,dim_less_curve,bin):\n",
    "\n",
    "        # Define target points for splits\n",
    "        target_points = np.linspace(0, 1, bin+1)\n",
    "\n",
    "        # normalized time\n",
    "        normalized_time = np.linspace(0, 1,len(dim_less_curve))\n",
    "\n",
    "        # Create interpolation function based on existing data points\n",
    "        interpolation_func = interp1d(normalized_time, dim_less_curve, kind='linear', fill_value=\"extrapolate\")\n",
    "\n",
    "        # Interpolate values at target points\n",
    "        interpolated_values = interpolation_func(target_points)\n",
    "\n",
    "        return interpolated_values            \n",
    "        \n",
    "    ################################################\n",
    "    def compute_intermittency(self,series):\n",
    "            \n",
    "        total_timesteps = len(series)\n",
    "        wet = series>0\n",
    "        transistions = (wet[:-1] != wet[1:]).sum()\n",
    "\n",
    "        intermittency = transistions/total_timesteps\n",
    "\n",
    "        return intermittency \n",
    "\n",
    "    def compute_rcg_idx(self,series):\n",
    "            \n",
    "        # Culmitative sum\n",
    "        cumulative_sum = np.cumsum(series)\n",
    "\n",
    "        # Normalize\n",
    "        cumulative_sum /= cumulative_sum[-1]\n",
    "\n",
    "        #first idx over 0.5\n",
    "        idx = np.argmax(cumulative_sum>0.5)\n",
    "\n",
    "        return idx\n",
    "\n",
    "    def compute_rcg(self, series, suffix):\n",
    "\n",
    "        if suffix in ['_dblnorm', '_norm']:\n",
    "            series = np.diff(series, prepend=0)\n",
    "            \n",
    "        n = len(series)\n",
    "        positions = np.arange(n)\n",
    "        total_mass = series.sum()\n",
    "        rcg_idx = int(np.round(np.sum(positions * series) / total_mass))\n",
    "        rcg = rcg_idx / n\n",
    "        return rcg\n",
    "    \n",
    "    def compute_rcg_interpolated(self, series, suffix):\n",
    "        if suffix in ['_dblnorm', '_norm']:\n",
    "            series = np.diff(series, prepend=0)\n",
    "\n",
    "        n = len(series)\n",
    "        if n < 2:\n",
    "            return np.nan  # Not enough data to interpolate\n",
    "\n",
    "        total_mass = series.sum()\n",
    "        if total_mass == 0:\n",
    "            print(f\"Zero mass encountered in event: {series}\")\n",
    "            return np.nan  # Avoid division by zero\n",
    "\n",
    "        positions = np.arange(n)\n",
    "        weighted_sum = np.sum(positions * series)\n",
    "        rcg = weighted_sum / total_mass / (n - 1)\n",
    "\n",
    "        return rcg\n",
    "\n",
    "\n",
    "    def calculate_pci(self, precip_series, suffix):\n",
    "        \"\"\"\n",
    "        Calculate the Precipitation Concentration Index (PCI) for a rainfall event.\n",
    "\n",
    "        Parameters:\n",
    "        precip_series (array-like): Rainfall depth values at regular intervals (e.g., every 5 minutes)\n",
    "\n",
    "        Returns:\n",
    "        float: PCI value\n",
    "        \"\"\"\n",
    "        if suffix in ['_dblnorm', '_norm']:\n",
    "            precip_series = np.diff(precip_series, prepend=0)\n",
    "            \n",
    "        precip_series = np.array(precip_series)\n",
    "\n",
    "        if precip_series.sum() == 0:\n",
    "            return 0  # Avoid division by zero, PCI is undefined for zero rainfall\n",
    "\n",
    "        numerator = np.sum(precip_series**2)\n",
    "        denominator = np.sum(precip_series)**2\n",
    "\n",
    "        pci = (numerator / denominator) * 100\n",
    "        return pci    \n",
    "    \n",
    "    \n",
    "    def compute_mass_dist_indicators(self, series, suffix, use_interpolation=True):\n",
    "        \"\"\"\n",
    "        Computes mass distribution indicators for a rainfall event.\n",
    "\n",
    "        Parameters:\n",
    "            series (array-like): Rainfall series.\n",
    "            suffix (str): Used to determine if diff should be applied.\n",
    "            use_interpolation (bool): Whether to compute indicators using interpolation or not.\n",
    "\n",
    "        Returns:\n",
    "            np.array: [M1, M2, M3, M4, M5]\n",
    "        \"\"\"\n",
    "        if suffix in ['_dblnorm', '_norm']:\n",
    "            series = np.diff(series, prepend=0)\n",
    "        series = pd.Series(series)\n",
    "\n",
    "        series = np.round(series, 6)\n",
    "        steps = len(series)\n",
    "        peak_idx = np.argmax(series)\n",
    "        cumulative_sum = np.cumsum(series)\n",
    "\n",
    "        total = cumulative_sum.iloc[-1]\n",
    "\n",
    "        # M1: mass before peak\n",
    "        if peak_idx == 0:\n",
    "            m1 = cumulative_sum[peak_idx] / total\n",
    "        else:\n",
    "            m1 = cumulative_sum[peak_idx] / (total - cumulative_sum[peak_idx - 1])\n",
    "\n",
    "        # M2: peak as fraction of total\n",
    "        m2 = series[peak_idx] / total\n",
    "\n",
    "        if use_interpolation:\n",
    "            # Normalized time for interpolation\n",
    "            time_norm = np.linspace(0, 1, steps)\n",
    "            cum_interp = interp1d(time_norm, cumulative_sum / total, kind='linear', fill_value=\"extrapolate\")\n",
    "            m3 = float(cum_interp(1/3))\n",
    "            m4 = float(cum_interp(0.3))\n",
    "            m5 = float(cum_interp(0.5))\n",
    "        else:\n",
    "            m3 = cumulative_sum[int(np.round(steps / 3)) - 1] / total\n",
    "            m4 = cumulative_sum[int(np.round(steps * 0.3)) - 1] / total\n",
    "            m5 = cumulative_sum[int(np.round(steps / 2)) - 1] / total\n",
    "\n",
    "        return np.array([m1, m2, m3, m4, m5])\n",
    "\n",
    "    def classify_BSC(self, series, suffix, plot=False):\n",
    "        \"\"\"\n",
    "        Classifies a rainfall event into a 4-digit binary shape code based on \n",
    "        the Terranova and Iaquinta (2011) method, for events of any length.\n",
    "\n",
    "        Parameters:\n",
    "            normalised_cumulative_event (np.array): \n",
    "                A NumPy array representing the normalised cumulative rainfall curve (0 to 1).\n",
    "            plot (bool): If True, generates a plot comparing actual and uniform cumulative curves.\n",
    "\n",
    "        Returns:\n",
    "            str: A 4-digit binary shape code (e.g., \"0110\").\n",
    "        \"\"\"\n",
    "        # Ensure the input is a NumPy array\n",
    "        \n",
    "        if suffix not in ['_norm', '_dblnorm']:\n",
    "            series = np.cumsum(series)\n",
    "        \n",
    "        normalised_cumulative_event = np.array(series)\n",
    "\n",
    "        # Get the number of time steps\n",
    "        n = len(normalised_cumulative_event)\n",
    "\n",
    "        # Compute indices which are found at the end of four quarters (roughly)\n",
    "        q1 = round(n * 0.25)\n",
    "        q2 = round(n * 0.50)\n",
    "        q3 = round(n * 0.75)\n",
    "\n",
    "        # Extract cumulative values at the end of each quarter, handling short events gracefully\n",
    "        vals_at_end_of_quarters = [\n",
    "            normalised_cumulative_event[min(q1, n-1)],  # End of Q1\n",
    "            normalised_cumulative_event[min(q2, n-1)],  # End of Q2\n",
    "            normalised_cumulative_event[min(q3, n-1)],  # End of Q3\n",
    "            normalised_cumulative_event[-1],            # End of Q4 (should be ~1)\n",
    "        ]\n",
    "\n",
    "        # Expected uniform cumulative values\n",
    "        expected_vals_at_end_of_quarters = [0.25, 0.50, 0.75, 1.0]\n",
    "\n",
    "        # Generate the binary shape code\n",
    "        binary_code = \"\".join([\"1\" if actual >= expected else \"0\" for actual, expected in zip(vals_at_end_of_quarters, expected_vals_at_end_of_quarters)])\n",
    "\n",
    "        # Optional Plot\n",
    "        if plot:\n",
    "            time_steps = np.linspace(0, 1, n)  # Normalized time axis\n",
    "            uniform_cumulative = np.linspace(0, 1, n)  # Perfectly uniform rainfall line\n",
    "\n",
    "            plt.figure(figsize=(8, 5))\n",
    "            plt.plot(time_steps, normalised_cumulative_event, label=\"Actual Cumulative Rainfall\", marker='o')\n",
    "            plt.plot(time_steps, uniform_cumulative, label=\"Uniform Rainfall Reference\", linestyle=\"--\", color=\"gray\")\n",
    "\n",
    "            # Mark quarter points\n",
    "            for i, (q, actual, expected) in enumerate(zip([q1, q2, q3, n-1], vals_at_end_of_quarters, expected_vals_at_end_of_quarters)):\n",
    "                plt.scatter(time_steps[q], actual, color=\"red\", zorder=3, label=f\"Q{i+1}: {binary_code[i]}\")\n",
    "\n",
    "            plt.xlabel(\"Normalized Time (0 to 1)\")\n",
    "            plt.ylabel(\"Cumulative Rainfall\")\n",
    "            plt.title(f\"Binary Shape Code: {binary_code}\")\n",
    "            plt.legend()\n",
    "            plt.grid()\n",
    "            plt.show()\n",
    "        \n",
    "        # Convert to index\n",
    "        binary_values = [int(b) for b in binary_code]            \n",
    "        # Define weights for front-loading emphasis\n",
    "        weights = [3, 1, -1, -3]  # Q1 = most front-loaded, Q4 = most back-loaded\n",
    "        # Compute the front-loading index\n",
    "        FLI = sum(w * b for w, b in zip(weights, binary_values))\n",
    "        \n",
    "        return np.array([binary_code,FLI])\n",
    "\n",
    "\n",
    "    def calculate_event_loading(self, series, suffix):\n",
    "        \"\"\"\n",
    "        Calculate event loading (EL) as the percent deviation in STH for a hypothetical,\n",
    "        mirrored storm from the original storm STH.\n",
    "        \"\"\"\n",
    "\n",
    "        def calculate_sth(storm):\n",
    "            mean_val = np.mean(storm)\n",
    "            std_val = np.std(storm)\n",
    "\n",
    "            if mean_val == 0:\n",
    "                return np.nan  # Return NaN to flag unusable case\n",
    "            return std_val / mean_val\n",
    "\n",
    "        if suffix in ['_dblnorm', '_norm']:\n",
    "            series = np.diff(series, prepend=0)\n",
    "\n",
    "        series = np.round(series, 6)\n",
    "\n",
    "        if np.all(series == 0):\n",
    "            print(\"Warning: All-zero series encountered. Returning NaN for event loading.\")\n",
    "            return np.nan\n",
    "\n",
    "        peak_index = np.argmax(series)\n",
    "        rising = series[:peak_index + 1]\n",
    "        mirrored_falling = rising[::-1][1:]\n",
    "        mirrored_storm = np.concatenate([rising, mirrored_falling])\n",
    "\n",
    "        sth_original = calculate_sth(series)\n",
    "        sth_mirrored = calculate_sth(mirrored_storm)\n",
    "\n",
    "        if np.isnan(sth_original) or sth_original == 0:\n",
    "            print(\"Warning: Invalid STH encountered. Returning NaN for event loading.\")\n",
    "            return np.nan\n",
    "\n",
    "        event_loading = ((sth_mirrored - sth_original) / sth_original) * 100\n",
    "        return event_loading \n",
    "\n",
    "    \n",
    "    def calculate_event_asymmetry(self, series, suffix):\n",
    "        \"\"\"Calculate asymmetry for a single rainfall event\"\"\"\n",
    "        if suffix in ['_dblnorm', '_norm']:\n",
    "            series = np.diff(series, prepend=0)                  \n",
    "        n = len(series)\n",
    "        if n < 3:  # Need at least 3 points for meaningful asymmetry\n",
    "            return np.nan\n",
    "\n",
    "        # Calculate empirical CDF (rank transform)\n",
    "        ranks = stats.rankdata(series)\n",
    "        U = (ranks - 0.5) / n\n",
    "\n",
    "        # Use lag-1 (consecutive 5-min intervals)\n",
    "        diff = U[:-2] - U[2:]\n",
    "\n",
    "        # Calculate A(k)\n",
    "        numerator = np.mean(diff**3)\n",
    "        denominator = np.mean(diff**2)**(3/2)\n",
    "\n",
    "        if denominator != 0:\n",
    "            return numerator/denominator\n",
    "        else:\n",
    "            return np.nan    \n",
    "\n",
    "    def compute_time_based_skewness(self, event_list, suffix):\n",
    "        time_delta = self.ts.data.index[1] - self.ts.data.index[0]\n",
    "        time_delta_minutes = time_delta.total_seconds() / 60\n",
    "\n",
    "        skewness_list = []\n",
    "        for event in event_list:\n",
    "            values = event.values.flatten()\n",
    "            if suffix in ['_dblnorm', '_norm']:\n",
    "                values = np.diff(values, prepend=0)\n",
    "            \n",
    "            n = len(values)\n",
    "            positions = np.arange(1, n + 1) * time_delta_minutes\n",
    "            total = values.sum()\n",
    "\n",
    "            # Center of mass\n",
    "            t_cg = np.sum(positions * values) / total\n",
    "\n",
    "            # Standard deviation\n",
    "            sigma_t = np.sqrt(np.sum(((positions - t_cg) ** 2) * values) / total)\n",
    "\n",
    "            # Skewness\n",
    "            skew = np.sum(((positions - t_cg) ** 3) * values) / (total * sigma_t**3)\n",
    "            skewness_list.append(skew)\n",
    "\n",
    "        return np.array(skewness_list)\n",
    "    \n",
    "    def compute_time_based_kurtosis(self, event_list, suffix):\n",
    "        time_delta = self.ts.data.index[1] - self.ts.data.index[0]\n",
    "        time_delta_minutes = time_delta.total_seconds() / 60\n",
    "\n",
    "        kurtosis_list = []\n",
    "        for event in event_list:\n",
    "            values = event.values.flatten()\n",
    "            if suffix in ['_dblnorm', '_norm']:\n",
    "                values = np.diff(values, prepend=0)            \n",
    "            n = len(values)\n",
    "            positions = np.arange(1, n + 1) * time_delta_minutes\n",
    "            total = values.sum()\n",
    "\n",
    "            # Center of mass\n",
    "            t_cg = np.sum(positions * values) / total\n",
    "\n",
    "            # Standard deviation\n",
    "            sigma_t = np.sqrt(np.sum(((positions - t_cg) ** 2) * values) / total)\n",
    "\n",
    "            # Kurtosis\n",
    "            kurt = np.sum(((positions - t_cg) ** 4) * values) / (total * sigma_t**4)\n",
    "            kurtosis_list.append(kurt)\n",
    "\n",
    "        return np.array(kurtosis_list)\n",
    "\n",
    "    def fourth_with_peak(self, series, suffix):\n",
    "        if suffix not in ['_norm', '_dblnorm']:\n",
    "            series = np.cumsum(series)\n",
    "\n",
    "        # culm value at splits\n",
    "        interpolated = self.interpolate_rainfall(series,3)\n",
    "\n",
    "        incremental =  np.diff(interpolated, prepend=0)\n",
    "        quintile = incremental.argmax()\n",
    "\n",
    "        return quintile\n",
    "\n",
    "    def fifth_with_peak(self, series, suffix):\n",
    "        if suffix not in ['_norm', '_dblnorm']:\n",
    "            series = np.cumsum(series)\n",
    "\n",
    "        # culm value at splits\n",
    "        interpolated = self.interpolate_rainfall(series,4)\n",
    "\n",
    "        incremental =  np.diff(interpolated, prepend=0)\n",
    "        quintile = incremental.argmax()\n",
    "\n",
    "        return quintile\n",
    "\n",
    "    def third_with_peak(self, series, suffix):\n",
    "        if suffix not in ['_norm', '_dblnorm']:\n",
    "            series = np.cumsum(series)\n",
    "\n",
    "        # culm value at splits\n",
    "        interpolated = self.interpolate_rainfall(series,2)\n",
    "\n",
    "        incremental =  np.diff(interpolated, prepend=0)\n",
    "        quintile = incremental.argmax()\n",
    "\n",
    "        return quintile\n",
    "\n",
    "\n",
    "    def calculate_skew_p(self, series, suffix):\n",
    "        \n",
    "        if suffix in ['_dblnorm', '_norm']:\n",
    "            series = np.diff(series, prepend=0)      \n",
    "        \n",
    "        # Find the length of rainfall event\n",
    "        n = len(series)\n",
    "\n",
    "        # Create time array with 5-minute intervals\n",
    "        # We want time points at 0, 5, 10, 15, ... minutes\n",
    "        t = np.arange(n) * 5  # This creates proper 5-minute intervals\n",
    "\n",
    "        # Find the time to the peak (in minutes)\n",
    "        peak_idx = np.argmax(series)\n",
    "        t_peak = t[peak_idx]\n",
    "\n",
    "        # Calculate normalized time differences\n",
    "        normalized_diff = (t - t_peak)/t[-1]  # normalize by total duration\n",
    "        cubed_diff = normalized_diff**3\n",
    "        skew_p = np.mean(cubed_diff)\n",
    "\n",
    "        return skew_p  \n",
    "    \n",
    "    def calculate_event_dry_ratio (self, series):\n",
    "        zeroes = np.count_nonzero(series==0)\n",
    "        event_dry_ratio = zeroes/len(series) * 100\n",
    "        return event_dry_ratio    \n",
    "    \n",
    "    def high_low_zone_indicators(self, series, suffix):\n",
    "        if suffix in ['_dblnorm', '_norm']:\n",
    "            series = np.diff(series, prepend=0)    \n",
    "\n",
    "        mean_intensity = series.mean()\n",
    "        indices_above = np.where(series > mean_intensity)[0]\n",
    "        indices_below = np.where(series < mean_intensity)[0]\n",
    "\n",
    "        # Proportion of time in high/low intensity zones\n",
    "        frac_time_in_high_intensity_zone = len(indices_above) / len(series) * 100\n",
    "        frac_time_in_low_intensity_zone = len(indices_below) / len(series) * 100\n",
    "\n",
    "        # Fraction of rainfall in high intensity zone\n",
    "        frac_rain_in_high_intensity_zone = series[indices_above].sum() / series.sum() * 100 if series.sum() > 0 else 0\n",
    "\n",
    "        # Mean intensity in high intensity zone â€” handle empty case\n",
    "        if len(indices_above) > 0:\n",
    "            mean_intensity_high_intensity_zone = series[indices_above].mean()\n",
    "        else:\n",
    "            mean_intensity_high_intensity_zone = 0  # or np.nan if you want to signal \"undefined\"\n",
    "\n",
    "        return np.array([\n",
    "            frac_time_in_high_intensity_zone,\n",
    "            frac_time_in_low_intensity_zone,\n",
    "            frac_rain_in_high_intensity_zone,\n",
    "            mean_intensity_high_intensity_zone\n",
    "        ])\n",
    "\n",
    "       \n",
    "    def calculate_nrmse_peak(self, series, suffix):\n",
    "        \n",
    "        \"\"\"\n",
    "        In this example:\n",
    "            The NRMSE calculation:\n",
    "            Takes the value at each time step\n",
    "            Computes how much each value differs from the peak (100 rainfall value)\n",
    "            Squares these differences\n",
    "            Takes the average\n",
    "            Takes the square root\n",
    "            Normalizes by total rainfall\n",
    "            \n",
    "            This value tells us how concentrated the rainfall is around its peak. A lower value (like this one) indicates \n",
    "            the rainfall is relatively concentrated around the peak, while a higher value would indicate the rainfall is more \n",
    "            spread out over time.            \n",
    "        \"\"\"\n",
    "        if suffix in ['_dblnorm', '_norm']:\n",
    "            series = np.diff(series, prepend=0) \n",
    "        series = np.round(series,6)    \n",
    "        # Array of rainfall values\n",
    "        pi = np.array(series)\n",
    "        # Finds the peak\n",
    "        ppeak = np.max(pi)\n",
    "        # Finds the total rainfall\n",
    "        P = np.sum(pi)\n",
    "        # Finds the length of the rainfall event\n",
    "        n = len(pi)\n",
    "        # Root-mean-square error between each ordinate (i.e. timestep) and the peak\n",
    "        rmse = np.sqrt(np.sum((pi - ppeak)**2) / n)\n",
    "        # Normalization by total rainfall\n",
    "        nrmse = rmse / P\n",
    "        return round(nrmse,2)\n",
    "\n",
    "\n",
    "    def gini_coef(self, series, suffix):\n",
    "        \"\"\"Compute the Gini coefficient in O(n) time without sorting or large memory allocations.\"\"\"\n",
    "        if suffix in ['_dblnorm', '_norm']:\n",
    "            series = np.diff(series, prepend=0)      \n",
    "        \n",
    "        n = len(series)\n",
    "        \n",
    "        series = series.flatten()\n",
    "        \n",
    "        if n == 0 or np.all(series == 0): \n",
    "            return 0  # Handle empty or zero arrays safely\n",
    "\n",
    "        abs_diffs = np.sum(np.abs(series[:, None] - series))  # Efficient pairwise sum\n",
    "        mean_value = np.mean(series)\n",
    "\n",
    "        return abs_diffs / (2 * n**2 * mean_value) if mean_value != 0 else 0\n",
    "\n",
    "    \n",
    "    def lorentz_asymmetry(self, series, suffix):\n",
    "        # https://doi.org/10.1016/j.jhydrol.2013.05.002\n",
    "        if suffix in ['_dblnorm', '_norm']:\n",
    "            series = np.diff(series, prepend=0)\n",
    "        series = np.round(series, 6)\n",
    "\n",
    "        if np.all(series == series[0]):\n",
    "            return np.nan  # or return 0.0 if you'd prefer a default value\n",
    "\n",
    "        n = len(series)\n",
    "        mean = np.mean(series)\n",
    "\n",
    "        lower = series[series < mean]\n",
    "        upper = series[series > mean]\n",
    "\n",
    "        if len(lower) == 0 or len(upper) == 0:\n",
    "            return np.nan  # asymmetry is undefined in this case\n",
    "\n",
    "        m = len(lower)\n",
    "        x_m = lower.max()\n",
    "        x_m1 = upper.min()\n",
    "\n",
    "        delta = (mean - x_m) / (x_m1 - x_m)\n",
    "        lorentz = (m + delta) / n + (lower.mean() + delta * x_m1) / np.sum(series)\n",
    "\n",
    "        return lorentz\n",
    "    \n",
    "    \n",
    "    def compute_frac_in_quarters(self, series, suffix, interpolate=False, target_length=20):\n",
    "        \"\"\"\n",
    "        Computes the fraction of rainfall in each of four time quarters, with optional interpolation\n",
    "        to ensure equal time slices. Applies necessary transformations depending on suffix.\n",
    "\n",
    "        Parameters:\n",
    "            series (array-like): Rainfall data (intensity or cumulative).\n",
    "            suffix (str): Used to determine whether to convert from cumulative.\n",
    "            interpolate (bool): Whether to interpolate to equal time quarters (recommended).\n",
    "            target_length (int): Number of points to interpolate to (must be divisible by 4).\n",
    "\n",
    "        Returns:\n",
    "            np.array: Four values representing % of rainfall in each time quarter.\n",
    "        \"\"\"\n",
    "        series = np.asarray(series)\n",
    "\n",
    "        # Convert to intensity if needed\n",
    "        if suffix in ['_dblnorm', '_norm']:\n",
    "            series = np.diff(series, prepend=0)\n",
    "\n",
    "        total = series.sum()\n",
    "        if total == 0:\n",
    "            return np.array([np.nan, np.nan, np.nan, np.nan])\n",
    "\n",
    "        if interpolate:\n",
    "            if target_length % 4 != 0:\n",
    "                raise ValueError(\"target_length must be divisible by 4\")\n",
    "\n",
    "            # Interpolate rainfall to fixed time resolution\n",
    "            x_old = np.linspace(0, 1, len(series))\n",
    "            x_new = np.linspace(0, 1, target_length)\n",
    "            interp_series = np.interp(x_new, x_old, series)\n",
    "\n",
    "            # Slice into 4 equal time quarters\n",
    "            quarter_len = target_length // 4\n",
    "            quarters = [interp_series[i * quarter_len : (i + 1) * quarter_len] for i in range(4)]\n",
    "            frac_in_quarters = [round(q.sum() / interp_series.sum() * 100, 1) for q in quarters]\n",
    "\n",
    "        else:\n",
    "            # Fallback to original fixed-index slicing (may be uneven if len(series) not divisible by 4)\n",
    "            n = len(series)\n",
    "            q1, q2, q3 = n // 4, n // 2, 3 * n // 4\n",
    "\n",
    "            quarter_1 = series[:q1]\n",
    "            quarter_2 = series[q1:q2]\n",
    "            quarter_3 = series[q2:q3]\n",
    "            quarter_4 = series[q3:]\n",
    "\n",
    "            if len(quarter_1) == len(quarter_2) == len(quarter_3) == len(quarter_4):\n",
    "                frac_q1 = round(quarter_1.sum() / total * 100, 1)\n",
    "                frac_q2 = round(quarter_2.sum() / total * 100, 1)\n",
    "                frac_q3 = round(quarter_3.sum() / total * 100, 1)\n",
    "                frac_q4 = round(quarter_4.sum() / total * 100, 1)\n",
    "                frac_in_quarters = [frac_q1, frac_q2, frac_q3, frac_q4]\n",
    "            else:\n",
    "                frac_in_quarters = [np.nan, np.nan, np.nan, np.nan]\n",
    "\n",
    "        return np.array(frac_in_quarters)\n",
    "\n",
    "\n",
    "\n",
    "    def calc_dX_with_interpolation(self, series, percentile, suffix, plot=False):\n",
    "        \"\"\"\n",
    "        Calculates the time (% of event duration) at which a given percentile\n",
    "        of cumulative rainfall is reached, using linear interpolation.\n",
    "        \"\"\"\n",
    "\n",
    "        if suffix not in ['_norm', '_dblnorm']:\n",
    "            series = np.cumsum(series)\n",
    "\n",
    "        n = len(series)\n",
    "        time_percent = np.linspace(0, 100, n)\n",
    "        step_size = 100 / (n - 1) if n > 1 else 100  # Handle degenerate case\n",
    "\n",
    "        percentile *= series[-1] \n",
    "        below = np.where(series < percentile)[0]\n",
    "        above = np.where(series >= percentile)[0]\n",
    "\n",
    "        if len(below) > 0 and len(above) > 0:\n",
    "            i_below = below[-1]\n",
    "            i_above = above[0]\n",
    "\n",
    "            x1, y1 = time_percent[i_below], series[i_below]\n",
    "            x2, y2 = time_percent[i_above], series[i_above]\n",
    "\n",
    "        elif len(below) == 0 and series[0] >= percentile:\n",
    "            # Edge case: percentile is hit in the first timestep\n",
    "            x1, y1 = 0, 0\n",
    "            x2, y2 = step_size, series[0]  # Now x2 is the end of the first step\n",
    "\n",
    "        else:\n",
    "            if plot:\n",
    "                print(\"Interpolation not possible: percentile not reached.\")\n",
    "                plt.plot(time_percent, series)\n",
    "                plt.axhline(percentile, color='red', linestyle='--')\n",
    "                plt.title(\"Cumulative rainfall with missing percentile\")\n",
    "                plt.xlabel(\"Time (% of duration)\")\n",
    "                plt.ylabel(\"Cumulative rainfall\")\n",
    "                plt.grid(True)\n",
    "                plt.show()\n",
    "            return None\n",
    "\n",
    "        slope = (y2 - y1) / (x2 - x1)\n",
    "        x_at_percentile = x1 + (percentile - y1) / slope\n",
    "\n",
    "        if plot:\n",
    "            plt.figure(figsize=(8, 4))\n",
    "            plt.plot(time_percent, series, label='Cumulative rainfall')\n",
    "            plt.axhline(percentile, color='red', linestyle='--', label=f'{int(percentile*100)}th percentile')\n",
    "            plt.plot([x1, x2], [y1, y2], 'ko-', label='Interpolation points')\n",
    "            plt.axvline(x_at_percentile, color='green', linestyle='--', label='Interpolated time')\n",
    "            plt.scatter([x_at_percentile], [percentile], color='green', zorder=5)\n",
    "            plt.title(f'Percentile Time Calculation ({int(x_at_percentile)})')\n",
    "            plt.xlabel('Time (% of duration)')\n",
    "            plt.ylabel('Cumulative Rainfall')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        return x_at_percentile\n",
    "\n",
    "    def calc_ARR_thirds(self, series, suffix):\n",
    "        \"\"\"\n",
    "        Classifies a normalised rainfall event into one of three ARR thirds \n",
    "        based on when 50% of rainfall occurs.\n",
    "\n",
    "        Returns:\n",
    "            1 if < 40% of time,\n",
    "            2 if between 40% and 60%,\n",
    "            3 if > 60%\n",
    "        \"\"\"\n",
    "        percentile = 0.5  # 50% of total rainfall\n",
    "        time_for_percentile = self.calc_dX_with_interpolation(series, percentile, suffix)\n",
    "\n",
    "        if time_for_percentile is None:\n",
    "            return None  # Optional: handle edge case if interpolation fails\n",
    "\n",
    "        if time_for_percentile < 40:\n",
    "            return 1\n",
    "        elif 40 <= time_for_percentile <= 60:\n",
    "            return 2\n",
    "        elif time_for_percentile > 60:\n",
    "            return 3\n",
    "\n",
    "    def compute_thirds_rcg_interpolated(self, event_series, suffix):\n",
    "        # Compute the center of gravity (CoM) for the event series\n",
    "        rcg = self.compute_rcg_interpolated(event_series, suffix)\n",
    "\n",
    "        # Classify the CoM fractional value into thirds\n",
    "        def classify_rcg_fraction(rcg_value):\n",
    "            if rcg_value < 1/3:\n",
    "                return 1\n",
    "            elif rcg_value < 2/3:\n",
    "                return 2\n",
    "            else:\n",
    "                return 3\n",
    "\n",
    "        # Return the classification for the event\n",
    "        return classify_rcg_fraction(rcg)\n",
    "    \n",
    "    \n",
    "    def find_heaviest_run_half(self, series, suffix, threshold=0.8):\n",
    "        \"\"\"\n",
    "        Determine whether the first or second half of a rainfall event contains the heaviest run.\n",
    "\n",
    "        Parameters:\n",
    "        - series: 1D array-like of precipitation values (e.g. mm per 5 min).\n",
    "        - threshold: Rainfall threshold to define a \"run\".\n",
    "\n",
    "        Returns:\n",
    "        - 'first_half', 'second_half', or 'both_halves' depending on where the heaviest run is.\n",
    "        - start and end index of the heaviest run.\n",
    "        \"\"\"\n",
    "        if suffix in ['_dblnorm', '_norm']:\n",
    "            series = np.diff(series, prepend=0)\n",
    "        \n",
    "        series = np.asarray(series)\n",
    "        above = series > threshold\n",
    "\n",
    "        # Identify run boundaries\n",
    "        run_ids = np.zeros_like(series, dtype=int)\n",
    "        run_ids[1:] = (above[1:] != above[:-1]).cumsum()\n",
    "\n",
    "        # Get runs where above threshold\n",
    "        valid_runs = []\n",
    "        for run_id in np.unique(run_ids[above]):\n",
    "            idx = np.where(run_ids == run_id)[0]\n",
    "            run_total = series[idx].sum()\n",
    "            valid_runs.append((idx[0], idx[-1], run_total))\n",
    "\n",
    "        if not valid_runs:\n",
    "            return None, None, None\n",
    "\n",
    "        # Find the heaviest run\n",
    "        start_idx, end_idx, _ = max(valid_runs, key=lambda x: x[2])\n",
    "\n",
    "        # Determine midpoint index\n",
    "        mid = len(series) // 2\n",
    "        if start_idx <= mid <= end_idx:\n",
    "            half = 'both_halves'\n",
    "        elif end_idx < mid:\n",
    "            half = 'first_half'\n",
    "        else:\n",
    "            half = 'second_half'\n",
    "\n",
    "        return half, start_idx, end_idx\n",
    "    \n",
    "    def calculate_tci(self, series, suffix):\n",
    "        if suffix in ['_dblnorm', '_norm']:\n",
    "            series = np.diff(series, prepend=0)\n",
    "\n",
    "        rainfall = np.array(series)\n",
    "        T = len(series)  # Total duration (number of time steps)\n",
    "        P = np.sum(series)  # Total rainfall\n",
    "\n",
    "        if P == 0:\n",
    "            return 0, None, [0] * T  # If no rainfall, TCI is zero\n",
    "\n",
    "        tci_values = []  # To store TCI for each hypothetical temporal center\n",
    "\n",
    "        # Step 5: Iterate over each possible temporal center\n",
    "        for center in range(T):\n",
    "            # Step 1: Mark the chosen time point as the hypothetical temporal center\n",
    "            sorted_indices = [center]\n",
    "\n",
    "            # Step 2: Rank the remaining time points based on proximity & rainfall\n",
    "            remaining_indices = list(range(T))\n",
    "            remaining_indices.remove(center)\n",
    "\n",
    "            # Sort remaining indices by distance from the center, prioritizing higher rainfall\n",
    "            remaining_indices.sort(key=lambda i: (abs(i - center), -rainfall[i]))\n",
    "            sorted_indices.extend(remaining_indices)\n",
    "\n",
    "            # Step 3: Compute cumulative rainfall following this order\n",
    "            cumulative_rainfall = np.cumsum(rainfall[sorted_indices])\n",
    "            cumulative_time = np.arange(1, T + 1)  # Time from 1 to T\n",
    "\n",
    "            # Step 4: Compute TCI\n",
    "            actual_curve = np.trapz(cumulative_rainfall, cumulative_time)  # Area under actual curve\n",
    "            reference_line = 0.5 * T * P  # Area under the reference straight line\n",
    "            dA = actual_curve - reference_line  # Difference in area\n",
    "            A = 0.5 * T * P  # Area of the reference triangle\n",
    "\n",
    "            tci = dA / A  # TCI for this center\n",
    "            tci_values.append(tci)\n",
    "\n",
    "        # Step 5: Identify the max TCI and corresponding temporal center\n",
    "        max_tci = max(tci_values)\n",
    "        best_center = np.argmax(tci_values)\n",
    "\n",
    "        return max_tci# , best_center, tci_values\n",
    "\n",
    "    \n",
    "    def get_metrics(self):\n",
    "        # These only apply to raw events\n",
    "        self.metrics['total_precip'] = np.array([e.sum().values[0] for e in self.ts.raw_events])\n",
    "        self.metrics[\"duration\"] = np.array([len(e)*temp_res for e in self.ts.raw_events])     \n",
    "        self.metrics[\"I30\"] = np.array([e.rolling(window=\"30min\").sum().max().values[0] for e in self.ts.raw_events])/30   \n",
    "        self.metrics[\"time_to_peak\"] = np.array([((e.idxmax().values[0]- e.index[0]).total_seconds()/60) for e in self.ts.raw_events])\n",
    "        self.metrics[\"peak_position_ratio\"] = self.metrics[\"time_to_peak\"]/self.metrics[\"duration\"]\n",
    "        \n",
    "        event_sets = {\n",
    "            \"\": self.ts.raw_events,\n",
    "            \"_DMC_10\": self.ts.DMCs,\n",
    "#             \"_DMC_100\": self.ts.DMCs_100,\n",
    "            \"_dblnorm\": self.ts.double_normalised_events,\n",
    "#             \"_norm\": self.ts.normalised_events\n",
    "        }\n",
    "\n",
    "        for suffix, events in event_sets.items():\n",
    "            print(suffix)\n",
    "            if suffix in ['_norm', '_dblnorm']:\n",
    "                self.metrics[f\"max_intensity{suffix}\"] = np.array([np.diff(e.iloc[:, 0].to_numpy(), prepend=0).max() for e in events])\n",
    "                self.metrics[f\"mean_intensity{suffix}\"] = np.array([np.diff(e.iloc[:, 0].to_numpy(), prepend=0).mean() for e in events])\n",
    "                self.metrics[f\"min_intensity{suffix}\"] = np.array([np.diff(e.iloc[:, 0].to_numpy(), prepend=0).min() for e in events])\n",
    "                self.metrics[f\"std{suffix}\"] = np.array([np.diff(e.iloc[:, 0].to_numpy(), prepend=0).std() for e in events])\n",
    "                self.metrics[f\"skewness{suffix}\"] = np.array([skew(np.diff(e.iloc[:, 0].to_numpy(), prepend=0), bias=False) for e in events])\n",
    "                self.metrics[f\"kurtosis{suffix}\"] = np.array([skew(np.diff(e.iloc[:, 0].to_numpy(), prepend=0), bias=False) for e in events])\n",
    "            else:\n",
    "                self.metrics[f\"std{suffix}\"] = np.array([e.std().values[0] for e in events])\n",
    "                self.metrics[f\"max_intensity{suffix}\"] = np.array([e.max().values[0] for e in events])\n",
    "                self.metrics[f\"mean_intensity{suffix}\"] = np.array([e.mean().values[0] for e in events]) \n",
    "                self.metrics[f\"min_intensity{suffix}\"] = np.array([e.min().values[0] for e in events])\n",
    "                self.metrics[f\"cv{suffix}\"] = self.metrics[f\"std{suffix}\"] / self.metrics[f\"mean_intensity{suffix}\"]\n",
    "                self.metrics[f\"skewness{suffix}\"] = np.array([e.skew().values[0] for e in events])\n",
    "                self.metrics[f\"kurtosis{suffix}\"] = np.array([e.kurtosis().values[0] for e in events])       \n",
    "            \n",
    "            self.metrics[f\"cv{suffix}\"] = self.metrics[f\"std{suffix}\"] / self.metrics[f\"mean_intensity{suffix}\"]                \n",
    "                \n",
    "            self.metrics[f\"relative_amp{suffix}\"] = (self.metrics[f\"max_intensity{suffix}\"] - self.metrics[f\"min_intensity{suffix}\"])/self.metrics[f\"mean_intensity{suffix}\"]\n",
    "#             self.metrics[f\"relative_amp_scaled{suffix}\"] = [(np.max(vals := e.iloc[:, 0].to_numpy()[e.iloc[:, 0].to_numpy() > 0]) - np.min(vals)) / np.mean(vals) if np.any(e.iloc[:, 0].to_numpy() > 0) else np.nan for e in events]\n",
    "            self.metrics[f\"peak_mean_ratio{suffix}\"] = self.metrics[f\"max_intensity{suffix}\"]/self.metrics[f\"mean_intensity{suffix}\"]\n",
    "            self.metrics[f\"peak_mean_ratio_scaled{suffix}\"] = [np.max(e.iloc[:, 0].to_numpy() / np.mean(e.iloc[:, 0].to_numpy()))\n",
    "                    for e in events]\n",
    "            \n",
    "            self.metrics[f\"PCI{suffix}\"] = np.array([self.calculate_pci(e.iloc[:, 0].to_numpy(), suffix) for e in events])\n",
    "            self.metrics[f\"TCI{suffix}\"] = np.array([self.calculate_tci(e.iloc[:, 0].to_numpy(), suffix) for e in events])\n",
    "            self.metrics[f\"asymm_d{suffix}\"] = np.array([self.calculate_event_asymmetry(e.iloc[:, 0].to_numpy(), suffix) for e in events])\n",
    "            self.metrics[f\"Event Loading{suffix}\"] = np.array([self.calculate_event_loading(e.values, suffix) for e in events]) \n",
    "            self.metrics[f\"NRMSE_P{suffix}\"] = np.array([self.calculate_nrmse_peak(e.iloc[:, 0].to_numpy(), suffix) for e in events])\n",
    "            self.metrics[f\"skewp{suffix}\"] = np.array([self.calculate_skew_p(e.iloc[:, 0].to_numpy(), suffix) for e in events])\n",
    "            \n",
    "            self.metrics[f\"gini{suffix}\"] = np.array([self.gini_coef(e.iloc[:, 0].to_numpy(), suffix) for e in events])\n",
    "            self.metrics[f\"lorentz_asymetry{suffix}\"] = np.array([self.lorentz_asymmetry(e.values, suffix) for e in events])  \n",
    "            \n",
    "            temp = np.array([self.find_heaviest_run_half(e.iloc[:, 0].to_numpy(), suffix) for e in events])\n",
    "            self.metrics[f\"heaviest_half{suffix}\"] = temp[:, 0]\n",
    "                       \n",
    "            self.metrics[f\"intermittency{suffix}\"] = np.array([self.compute_intermittency(e.values) for e in events])\n",
    "            self.metrics[f\"event_dry_ratio{suffix}\"] = np.array([self.calculate_event_dry_ratio(e.values) for e in events])\n",
    "            \n",
    "            self.metrics[f'ni{suffix}']= self.metrics[f\"max_intensity{suffix}\"]/self.metrics[f\"mean_intensity{suffix}\"]\n",
    "            self.metrics[f\"time_skewness{suffix}\"] = self.compute_time_based_skewness(events, suffix)\n",
    "            self.metrics[f\"time_kurtosis{suffix}\"] = self.compute_time_based_kurtosis(events, suffix)\n",
    "            \n",
    "            self.metrics[f\"centre_gravity{suffix}\"] = np.array([self.compute_rcg(e.iloc[:, 0].to_numpy(), suffix) for e in events])\n",
    "            self.metrics[f\"centre_gravity_interpolated{suffix}\"] = np.array([self.compute_rcg_interpolated(e.iloc[:, 0].to_numpy(), suffix) for e in events])\n",
    "            \n",
    "            # Mass distribution indicators m1â€“m5\n",
    "            temp = np.array([self.compute_mass_dist_indicators(e.iloc[:, 0].to_numpy(), suffix, False) for e in events])\n",
    "            self.metrics[f\"m1{suffix}\"] = temp[:, 0]\n",
    "            self.metrics[f\"m2{suffix}\"] = temp[:, 1]\n",
    "            self.metrics[f\"m3{suffix}\"] = temp[:, 2]\n",
    "            self.metrics[f\"m4{suffix}\"] = temp[:, 3]\n",
    "            self.metrics[f\"m5{suffix}\"] = temp[:, 4]\n",
    "            \n",
    "            temp = np.array([self.compute_mass_dist_indicators(e.iloc[:, 0].to_numpy(), suffix, True) for e in events])\n",
    "            self.metrics[f\"m1_wi{suffix}\"] = temp[:, 0]\n",
    "            self.metrics[f\"m2_wi{suffix}\"] = temp[:, 1]\n",
    "            self.metrics[f\"m3_wi{suffix}\"] = temp[:, 2]\n",
    "            self.metrics[f\"m4_wi{suffix}\"] = temp[:, 3]\n",
    "            self.metrics[f\"m5_wi{suffix}\"] = temp[:, 4]            \n",
    "\n",
    "            temp = np.array([self.compute_frac_in_quarters(e.iloc[:, 0].to_numpy(), suffix) for e in events])\n",
    "            self.metrics[f\"frac_q1{suffix}\"] = temp[:,0]\n",
    "            self.metrics[f\"frac_q2{suffix}\"] = temp[:,1]\n",
    "            self.metrics[f\"frac_q3{suffix}\"] = temp[:,2]\n",
    "            self.metrics[f\"frac_q4{suffix}\"] = temp[:,3]\n",
    "            \n",
    "            temp = np.array([self.compute_frac_in_quarters(e.iloc[:, 0].to_numpy(), suffix, True) for e in events])\n",
    "            self.metrics[f\"frac_q1_wi_{suffix}\"] = temp[:,0]\n",
    "            self.metrics[f\"frac_q2_wi_{suffix}\"] = temp[:,1]\n",
    "            self.metrics[f\"frac_q3_wi_{suffix}\"] = temp[:,2]\n",
    "            self.metrics[f\"frac_q4_wi_{suffix}\"] = temp[:,3]              \n",
    "\n",
    "            # Indicators based on dividing event into low and high parts\n",
    "            temp = np.array([self.high_low_zone_indicators(e.iloc[:, 0].to_numpy(), suffix) for e in events])\n",
    "            self.metrics[f\"% time HIZ{suffix}\"] = temp[:,0]\n",
    "            self.metrics[f\"% time LIZ{suffix}\"] = temp[:,1]\n",
    "            self.metrics[f\"% rain HIZ{suffix}\"] = temp[:,2]\n",
    "            self.metrics[f\"Mean Intensity HIZ{suffix}\"] = temp[:,3]\n",
    "\n",
    "            # Huff quantiles\n",
    "            self.metrics[f\"3rd_w_peak{suffix}\"] = np.array([self.third_with_peak(e.iloc[:, 0].to_numpy(), suffix) for e in events])\n",
    "            self.metrics[f\"4th_w_peak{suffix}\"] = np.array([self.fourth_with_peak(e.iloc[:, 0].to_numpy(), suffix) for e in events])\n",
    "            self.metrics[f\"5th_w_peak{suffix}\"] = np.array([self.fifth_with_peak(e.iloc[:, 0].to_numpy(), suffix) for e in events])\n",
    "            self.metrics[\"third_ppr\"] = np.select([self.metrics[\"peak_position_ratio\"] < 0.4, (self.metrics[\"peak_position_ratio\"] >= 0.4) & (self.metrics[\"peak_position_ratio\"] <= 0.6), self.metrics[\"peak_position_ratio\"] > 0.6],[0, 1, 2])\n",
    "\n",
    "            # third with highest percent rain\n",
    "            self.metrics[f\"3rd_ARR{suffix}\"] = np.array([self.calc_ARR_thirds(e.iloc[:, 0].to_numpy(), suffix) for e in events])\n",
    "            self.metrics[f\"3rd_rcg{suffix}\"] = np.array([self.compute_thirds_rcg_interpolated(e.iloc[:, 0].to_numpy(), suffix) for e in events])\n",
    "            \n",
    "            # D50 etc\n",
    "            self.metrics[f\"T25{suffix}\"] = np.array([self.calc_dX_with_interpolation(e.iloc[:, 0].to_numpy(), 0.25, suffix) for e in events])\n",
    "            self.metrics[f\"T50{suffix}\"] = np.array([self.calc_dX_with_interpolation(e.iloc[:, 0].to_numpy(), 0.50, suffix) for e in events])\n",
    "            self.metrics[f\"T75{suffix}\"] = np.array([self.calc_dX_with_interpolation(e.iloc[:, 0].to_numpy(), 0.75, suffix) for e in events])\n",
    "            self.metrics[f\"D50{suffix}\"] = np.array([self.calc_dX_with_interpolation(e.iloc[:, 0].to_numpy(), 0.5, suffix) for e in events])\n",
    "        \n",
    "            # calculate BSC\n",
    "            temp = np.array([self.classify_BSC(e.iloc[:, 0].to_numpy(), suffix, False) for e in events])         \n",
    "            self.metrics[f\"BSC{suffix}\"] = temp[:,0]\n",
    "            self.metrics[f\"BSC_Index{suffix}\"] = temp[:, 1].astype(int)\n",
    "        \n",
    "    def plot_boxplots(self, metrics):\n",
    "        n_plots = len(metrics)\n",
    "        cols = min(n_plots, 4)  # Max 4 columns\n",
    "        rows = np.int16(np.ceil(n_plots / 4))  # Determine number of rows\n",
    "\n",
    "        fig, axes = plt.subplots(rows, cols, figsize=(10, 8))  # Create subplots grid\n",
    "        axes = np.ravel(axes)  # Flatten to 1D for easy iteration\n",
    "\n",
    "        for i, metric in enumerate(metrics):\n",
    "            axes[i].boxplot(self.metrics[metric])  # Use correct subplot\n",
    "            axes[i].set_title(metric)\n",
    "\n",
    "        plt.tight_layout()  # Adjust layout for clarity\n",
    "    \n",
    "    def plot_histograms(self,metrics):\n",
    "        \n",
    "        n_plots = len(metrics)\n",
    "        cols = min(n_plots, 4)  # Max 4 columns\n",
    "        rows = np.int16(np.ceil(n_plots / 4))  # Determine number of rows\n",
    "\n",
    "        fig, axes = plt.subplots(rows, cols, figsize=(10, 8))  # Create subplots grid\n",
    "        axes = np.ravel(axes)  # Flatten to 1D for easy iteration\n",
    "\n",
    "        for i, metric in enumerate(metrics):\n",
    "            axes[i].hist(self.metrics[metric])  # Use correct subplot\n",
    "            axes[i].set_title(metric)\n",
    "\n",
    "        plt.tight_layout()  # Adjust layout for clarity\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60090001",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "598120_precip_minute.csv\n",
      "Making the class so I am\n",
      "598120\n",
      "duplicate flags length: 0\n",
      "Number of missing time steps: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_events = []\n",
    "negatives=[]\n",
    "dest_dir = \"/nfs/a319/gy17m2a/Metrics/DanishRainData/\"\n",
    "files = [ f for f in os.listdir(dest_dir) if os.path.isfile(os.path.join(dest_dir,f)) ][0:1]\n",
    "for file in [\"598120_precip_minute.csv\"]:\n",
    "    print(file)\n",
    "    if pd.read_csv(f\"/nfs/a319/gy17m2a/Metrics/DanishRainData/{file}\").empty:\n",
    "        print(\"The CSV file has no data rows.\")\n",
    "    else:   \n",
    "        ts = precip_time_series(f\"/nfs/a319/gy17m2a/Metrics/DanishRainData/{file}\")\n",
    "        if len(ts.data[ts.data['precipitation (mm/hr)']<0]) >0:\n",
    "            print(\"Not including, still has negatives\")\n",
    "            negatives.append(file)\n",
    "        else:\n",
    "            ts.pad_and_resample('5')\n",
    "\n",
    "            # check if enough values\n",
    "            dt_index = ts.data.index\n",
    "            full_range = pd.date_range(start=dt_index.min(), end=dt_index.max(), freq='5T')\n",
    "            missing = full_range.difference(dt_index)\n",
    "            print(f\"Number of missing time steps: {len(missing)}\")\n",
    "            if len(missing) > 0:\n",
    "                print(\"First few missing timestamps:\")\n",
    "                print(missing[:10])\n",
    "\n",
    "            analysis = rainfall_analysis('11h', ts)\n",
    "\n",
    "            if ts.events != None:\n",
    "                analysis.get_metrics()\n",
    "                df = pd.DataFrame(analysis.metrics)\n",
    "                df['gauge_num'] = file[:6]\n",
    "                all_events.append(df)\n",
    "                with open(f'/nfs/a319/gy17m2a/Metrics/DanishRainDataPickles/{file}.pkl', 'wb') as f:\n",
    "                    pickle.dump(ts, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "                all_events_df = pd.concat(all_events)\n",
    "                all_events_df.to_csv(\"All_events_new.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3449ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(all_events_df))\n",
    "all_events_df = all_events_df[all_events_df['total_precip']>4]\n",
    "print(len(all_events_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfe51d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ts.events != None:\n",
    "    analysis.get_metrics()\n",
    "    df = pd.DataFrame(analysis.metrics)\n",
    "    df['gauge_num'] = file[:6]\n",
    "    all_events.append(df)\n",
    "    print(len(all_events))\n",
    "    with open(f'/nfs/a319/gy17m2a/Metrics/DanishRainDataPickles/{file}.pkl', 'wb') as f:\n",
    "        pickle.dump(ts, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    all_events_df = pd.concat(all_events)\n",
    "    all_events_df.to_csv(\"All_events_new.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2696a47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_and_resample(ts, freq = f'{temp_res}min' ,pad_value = 0):\n",
    "    # Resample the data to the specified frequency and pad missing values with pad_value\n",
    "    ts.data = ts.data.resample(freq).sum().fillna(pad_value)\n",
    "    ts.data *= 60 / temp_res\n",
    "    ts.padded = True\n",
    "    return ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7590a68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "event = ts.raw_events[2]\n",
    "find_heaviest_run_half(event.iloc[:, 0].to_numpy(), '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0db54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = precip_time_series(f\"/nfs/a319/gy17m2a/Metrics/DanishRainData/{file}\")\n",
    "pad_and_resample(ts, '5min')\n",
    "ts.data[:20]\n",
    "# ts.data are values in mm/min they are 0.05, 0, 0, 0, 0.10. I want to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c64e505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(all_events_df))\n",
    "# all_events_df = all_events_df[all_events_df['total_precip']>4]\n",
    "# print(len(all_events_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a658b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del all_events_df['min_intensity']\n",
    "# del all_events_df['BSC']\n",
    "# del all_events_df['BSC_Index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d46c891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_events_df.to_csv(\"three_gauge_events.csv\", index=False)\n",
    "# all_events_df = pd.read_csv(\"three_gauge_events.csv\")\n",
    "# all_events_df.copy=all_events_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8f1a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del all_events_df['BSC']\n",
    "# del all_events_df['BSC_DMC']\n",
    "# del all_events_df['mean_intensity_DMC']\n",
    "# del all_events_df['min_intensity_DMC']\n",
    "# all_events_df.rename(columns={\"third_class_max_percent_DMCs\": \"third_class_max_percent_DMC\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc4a442",
   "metadata": {},
   "source": [
    "## Create Correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d7b455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparison_df = compare_metrics_from_df(all_events_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6a620e",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_events_df = all_events_df[all_events_df['duration'] % 20 == 0]\n",
    "comparison_df = compare_metrics_from_df(filtered_events_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f652e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(figsize=(10, 8))\n",
    "# metric = comparison_df['metric'][2]\n",
    "\n",
    "# x = all_events_df[metric]\n",
    "# y = all_events_df[f'{metric}_DMC']\n",
    "\n",
    "# # Remove NaNs\n",
    "# mask = ~x.isna() & ~y.isna()\n",
    "# x = x[mask]\n",
    "# y = y[mask]\n",
    "\n",
    "# # Density plot\n",
    "# sns.histplot(x=x, y=y, bins=5, pmax=0.9, ax=ax, cmap=\"viridis\", cbar=False)\n",
    "\n",
    "# ax.set_title(metric)\n",
    "# ax.set_xlabel('Raw')\n",
    "# ax.set_ylabel('DMC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0f8f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=8, ncols=6, figsize=(20, 15))\n",
    "\n",
    "for ax_num, ax in enumerate(axs.flatten()):\n",
    "    if ax_num < len(comparison_df['metric']):\n",
    "        metric = comparison_df['metric'][ax_num]\n",
    "        \n",
    "        x = all_events_df[metric]\n",
    "        y = all_events_df[f'{metric}_DMC']\n",
    "        \n",
    "        # Remove NaNs\n",
    "        mask = ~x.isna() & ~y.isna()\n",
    "        x = x[mask]\n",
    "        y = y[mask]\n",
    "        \n",
    "        if metric == '4th_w_peak':\n",
    "            bins=4\n",
    "        elif metric == '5th_w_peak':\n",
    "            bins=5   \n",
    "        elif metric == 'ARR_Thirds' or metric =='3rd_w_peak' or metric == 'third_class_max_percent':\n",
    "            bins=3\n",
    "        elif metric == 'BSC_Index':\n",
    "            bins=5\n",
    "        else:\n",
    "            bins=30\n",
    "        \n",
    "        # Density plot\n",
    "        sns.histplot(x=x, y=y, bins=bins, pmax=0.9, ax=ax, cmap=\"plasma\", cbar=False)\n",
    "        \n",
    "        ax.set_title(metric)\n",
    "        ax.set_xlabel('Raw')\n",
    "        ax.set_ylabel('DMC')\n",
    "    else:\n",
    "        ax.set_axis_off()\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.savefig(\"DMC_Raw_Comparison.png\", facecolor='white', edgecolor='white', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e015d1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_cols = []\n",
    "for col in all_events_df.columns:\n",
    "    if not col.endswith('_DMC'):\n",
    "        raw_cols.append(col)\n",
    "all_events_df_raw = all_events_df[raw_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ffd6bf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "correlation_matrix = all_events_df_raw.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d22e640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the plot\n",
    "plt.figure(figsize=(20, 15))\n",
    "\n",
    "# Mask the upper triangle\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "# Draw group separators (e.g., after every 3 variables)\n",
    "group_sizes = [0, 9, 5]  # Adjust based on your actual groupings\n",
    "group_cuts = np.cumsum(group_sizes)\n",
    "\n",
    "for cut in group_cuts[:-1]:  # Don't draw line after the last group\n",
    "    plt.axhline(cut, color='black', linewidth=1)\n",
    "    plt.axvline(cut, color='black', linewidth=1)\n",
    "\n",
    "# Create heatmap\n",
    "sns.heatmap(correlation_matrix, mask=mask, cmap='coolwarm', annot=False, fmt=\".2f\", \n",
    "            linewidths=0.5, cbar=True, xticklabels=True, yticklabels=True)\n",
    "\n",
    "# Improve layout\n",
    "plt.xticks(rotation=90)  # Rotate x-axis labels for readability\n",
    "plt.yticks(rotation=0)  # Ensure y-axis labels are readable\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9324bfe6",
   "metadata": {},
   "source": [
    "### Principal Components Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5c2fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9438f8cc",
   "metadata": {},
   "source": [
    "### PCA 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606914ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix.to_csv(\"correlation.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4e2a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'three_gauge_events.csv'\n",
    "df = all_events_df\n",
    "# Display the head of the dataframe to understand its structure\n",
    "\n",
    "# Standardizing the data before PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(df)\n",
    "\n",
    "# Performing PCA\n",
    "pca = PCA()\n",
    "pca.fit(scaled_data)\n",
    "\n",
    "# Explained variance ratio\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "\n",
    "# Plotting the explained variance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(explained_variance) + 1), explained_variance, marker='o')\n",
    "plt.title('Explained Variance by Principal Components')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Variance Explained')\n",
    "plt.xticks(range(1, len(explained_variance) + 1))\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Cumulative explained variance\n",
    "cumulative_variance = explained_variance.cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67352689",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting the explained variance ratio\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(1, len(explained_variance) + 1), explained_variance, alpha=0.7, color='b', label='Explained Variance Ratio')\n",
    "plt.plot(range(1, len(explained_variance) + 1), cumulative_variance, marker='o', color='r', label='Cumulative Explained Variance')\n",
    "plt.title('PCA Explained Variance Ratio and Cumulative Explained Variance')\n",
    "plt.xlabel('Principal Components')\n",
    "plt.ylabel('Variance Explained')\n",
    "plt.xticks(range(1, len(explained_variance) + 1))\n",
    "plt.axhline(y=0.80, color='g', linestyle='--', label='80% Threshold')\n",
    "plt.axhline(y=0.90, color='orange', linestyle='--', label='90% Threshold')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e362bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming we have the PCA components and the original feature names\n",
    "# Let's extract the features contributing to the first 6 principal components\n",
    "\n",
    "# Assuming pca.components_ contains the PCA components and original_feature_names is a list of feature names\n",
    "# For demonstration, let's create a mock PCA components array and feature names\n",
    "pca_components = pca.components_[:10]\n",
    "original_feature_names = all_events_df.columns\n",
    "\n",
    "# For now, let's create mock data for demonstration\n",
    "np.random.seed(0)\n",
    "# pca_components = np.random.rand(6, len(explained_variance))  # Mock PCA components for 6 components\n",
    "# original_feature_names = [f'Feature {i+1}' for i in range(len(explained_variance))]  # Mock feature names\n",
    "\n",
    "# Creating a DataFrame to show the contribution of each feature to the first 6 components\n",
    "feature_contributions = pd.DataFrame(pca_components, columns=original_feature_names)\n",
    "feature_contributions.index = [f'PC {i+1}' for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccaa0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarizing the most influential features for the first 6 principal components\n",
    "# Finding the top contributing features for each principal component\n",
    "num_top_features = 20  # Number of top features to display\n",
    "summary = {}\n",
    "\n",
    "for i in range(6):  # For each principal component\n",
    "    top_features = feature_contributions.iloc[i].nlargest(num_top_features)\n",
    "    summary[f'PC {i+1}'] = top_features.index.tolist()\n",
    "\n",
    "summary_df = pd.DataFrame(summary)\n",
    "\n",
    "# Displaying the summary of top features for each principal component\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa81d907",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Drop any non-numeric or unwanted columns (e.g., identifiers)\n",
    "X = all_events_df.select_dtypes(include=[float, int])\n",
    "\n",
    "# Standardize\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# PCA\n",
    "pca = PCA(n_components=min(X.shape[1], 10))  # adjust number of components as needed\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Create DataFrame of PCA results\n",
    "pca_df = pd.DataFrame(X_pca, columns=[f\"PC{i+1}\" for i in range(X_pca.shape[1])])\n",
    "pca_df.index = all_events_df.index\n",
    "\n",
    "# Variance explained\n",
    "explained_var = pca.explained_variance_ratio_\n",
    "\n",
    "# Scree plot\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.barplot(x=[f\"PC{i+1}\" for i in range(len(explained_var))], y=explained_var)\n",
    "plt.title(\"Explained Variance by Principal Component\")\n",
    "plt.ylabel(\"Proportion of Variance\")\n",
    "plt.xlabel(\"Principal Component\")\n",
    "plt.show()\n",
    "\n",
    "# Optional: biplot or loading plot\n",
    "loadings = pd.DataFrame(pca.components_.T, index=X.columns, columns=pca_df.columns)\n",
    "\n",
    "# Plot first two PCs with loadings (biplot style)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=pca_df[\"PC1\"], y=pca_df[\"PC2\"])\n",
    "for i, var in enumerate(X.columns):\n",
    "    plt.arrow(0, 0, loadings[\"PC1\"][i]*2, loadings[\"PC2\"][i]*2,\n",
    "              color='r', alpha=1)\n",
    "    plt.text(loadings[\"PC1\"][i]*2.2, loadings[\"PC2\"][i]*2.2, var, color='r')\n",
    "\n",
    "plt.title(\"PCA Biplot (PC1 vs PC2)\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.grid(True)\n",
    "plt.axhline(0, color='grey', linewidth=0.5)\n",
    "plt.axvline(0, color='grey', linewidth=0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3900327",
   "metadata": {},
   "source": [
    "### Hierarchical clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb417995",
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered = cluster_from_correlation1(correlation_matrix, num_clusters=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4af810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for clustering and silhouette score calculation\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Prepare to calculate silhouette scores for different numbers of clusters\n",
    "range_n_clusters = range(2, 11)  # Testing from 2 to 10 clusters\n",
    "silhouette_scores = []\n",
    "\n",
    "# Calculate silhouette scores for each number of clusters\n",
    "for n_clusters in range_n_clusters:\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(correlation_matrix)\n",
    "    silhouette_avg = silhouette_score(correlation_matrix, cluster_labels)\n",
    "    silhouette_scores.append(silhouette_avg)\n",
    "\n",
    "# Plot the silhouette scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range_n_clusters, silhouette_scores, marker='o')\n",
    "plt.title('Silhouette Scores for Different Numbers of Clusters')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.xticks(range_n_clusters)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a94523",
   "metadata": {},
   "source": [
    "### Correlations with moments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c118381c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your moment metrics of interest\n",
    "moments = ['mean_intensity', 'std', 'kurtosis', 'skewness']\n",
    "moments = [m for m in moments if m in all_events_df.columns]\n",
    "print(moments)\n",
    "corr_matrix, pval_matrix = corr_and_pvalues(all_events_df[5:10])\n",
    "\n",
    "# Set up plot\n",
    "fig, axs = plt.subplots(ncols=len(moments), figsize=(20,10))\n",
    "vmin, vmax = -1, 1\n",
    "heatmaps = []\n",
    "\n",
    "for ax, moment in zip(axs, moments):\n",
    "    # Correlations for one column\n",
    "    col_corr = corr_matrix[[moment]].sort_values(by=moment, ascending=False)\n",
    "    col_pval = pval_matrix[[moment]].loc[col_corr.index]\n",
    "\n",
    "    # Annotate with asterisks for significance\n",
    "    annotations = col_corr.copy()\n",
    "    for i in range(len(annotations)):\n",
    "        r = col_corr.iloc[i, 0]\n",
    "        p = col_pval.iloc[i, 0]\n",
    "        annotations.iloc[i, 0] = f\"{r:.2f}\" + (\"*\" if p < 0.05 else \"\")\n",
    "\n",
    "    # Draw heatmap without colorbar\n",
    "    hm = sns.heatmap(col_corr[1:], annot=annotations[1:], fmt=\"\", cmap='coolwarm',\n",
    "                     vmin=vmin, vmax=vmax, ax=ax, cbar=False, linewidths=0.5,\n",
    "                     linecolor='lightgray', annot_kws={\"size\": 10})\n",
    "\n",
    "    ax.set_title(f'Correlation with {moment}')\n",
    "    heatmaps.append(hm)\n",
    "\n",
    "# Add shared colorbar\n",
    "cbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])\n",
    "fig.colorbar(heatmaps[-1].collections[0], cax=cbar_ax)\n",
    "plt.tight_layout(rect=[0, 0, 0.9, 1])\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
