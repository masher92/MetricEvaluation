{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3b918b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5192_svk_precip_minute.csv DanishRainData_SVK\n",
      "Making the class so I am\n",
      "5192\n",
      "duplicate flags length: 189\n",
      "Number of missing time steps: 0\n",
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'peak_position_ratio'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_25164/2100095154.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0manalysis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m             \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manalysis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'gauge_num'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nfs/a319/gy17m2a/Metrics/scripts/Run/ClassFunctions.py\u001b[0m in \u001b[0;36mget_metrics\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1381\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf\"% time HIZ{suffix}\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1382\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf\"% time LIZ{suffix}\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1383\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf\"% rain HIZ{suffix}\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1384\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf\"Mean Intensity HIZ{suffix}\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'peak_position_ratio'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from scipy.interpolate import interp1d\n",
    "import seaborn as sns\n",
    "import os\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import pickle\n",
    "import datetime \n",
    "import warnings\n",
    "import sys\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "from ClassFunctions import precip_time_series, rainfall_analysis\n",
    "# from PlottingFunctions import *\n",
    "\n",
    "file=\"5192_svk_precip_minute.csv\"\n",
    "directory = 'DanishRainData_SVK'\n",
    "\n",
    "print(file, directory)\n",
    "\n",
    "temp_res = 5\n",
    "\n",
    "all_events = []\n",
    "\n",
    "if pd.read_csv(f\"/nfs/a319/gy17m2a/Metrics/{directory}/{file}\").empty:\n",
    "    print(\"The CSV file has no data rows.\")\n",
    "else:   \n",
    "    ts = precip_time_series(f\"/nfs/a319/gy17m2a/Metrics/{directory}/{file}\")\n",
    "    if len(ts.data[ts.data['precipitation (mm/min)']<0]) >0:\n",
    "        print(\"Not including, still has negatives\")\n",
    "        negatives.append(file)\n",
    "    else:\n",
    "        ts.pad_and_resample(f'{temp_res}')\n",
    "\n",
    "        # check if enough values\n",
    "        dt_index = ts.data.index\n",
    "        full_range = pd.date_range(start=dt_index.min(), end=dt_index.max(), freq=f'{temp_res}T')\n",
    "        missing = full_range.difference(dt_index)\n",
    "        print(f\"Number of missing time steps: {len(missing)}\")\n",
    "        if len(missing) > 0:\n",
    "            print(\"First few missing timestamps:\")\n",
    "            print(missing[:10])\n",
    "\n",
    "        analysis = rainfall_analysis('11h', ts)\n",
    "\n",
    "        if ts.events != None:\n",
    "            with open(f'/nfs/a319/gy17m2a/Metrics/DanishRainDataPickles/{file}.pkl', 'wb') as f:\n",
    "                pickle.dump(ts, f, 4)\n",
    "            \n",
    "            analysis.get_metrics()\n",
    "            df = pd.DataFrame(analysis.metrics)\n",
    "            df['gauge_num'] = file.split('_')[0]\n",
    "            all_events.append(df)\n",
    "   \n",
    "            all_events_df = pd.concat(all_events)\n",
    "    \n",
    "            # Add start and end times\n",
    "            start_times = []\n",
    "            end_times = []\n",
    "            for timestamp in ts.events:\n",
    "                start_times.append(timestamp[0])\n",
    "                end_times.append(timestamp[1])\n",
    "            all_events_df['start_time'] =start_times\n",
    "            all_events_df['end_time'] =end_times\n",
    "    \n",
    "#             all_events_df.to_csv(f\"/nfs/a319/gy17m2a/Metrics/DanishRainData_Outputs/{temp_res}mins/All_events_{file}\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82e0711",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts.raw_events[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcde544",
   "metadata": {},
   "outputs": [],
   "source": [
    "self.metrics[f\"time_to_peak{suffix}\"] = np.array([((e.idxmax().values[0]- e.index[0]).total_seconds()/60) for e in events])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff30609b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from scipy.interpolate import interp1d\n",
    "import seaborn as sns\n",
    "import os\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import pickle\n",
    "import datetime \n",
    "import warnings\n",
    "import sys\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "from ClassFunctions import precip_time_series, rainfall_analysis\n",
    "sys.path.insert(0, '../')\n",
    "from PlottingFunctions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98f1e8c",
   "metadata": {},
   "source": [
    "## Create object containing rainfall events for one gauge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23094cfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "60090001",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "610400_precip_minute.csv\n",
      "Making the class so I am\n",
      "610400\n",
      "duplicate flags length: 0\n",
      "Number of missing time steps: 0\n",
      "\n",
      "_DMC_10\n",
      "_dblnorm\n"
     ]
    }
   ],
   "source": [
    "all_events = []\n",
    "negatives=[]\n",
    "dest_dir = \"/nfs/a319/gy17m2a/Metrics/DanishRainData/\"\n",
    "files = [ f for f in os.listdir(dest_dir) if os.path.isfile(os.path.join(dest_dir,f)) ][0:1]\n",
    "for file in [\"610400_precip_minute.csv\"]:\n",
    "    print(file)\n",
    "    if pd.read_csv(f\"/nfs/a319/gy17m2a/Metrics/DanishRainData/{file}\").empty:\n",
    "        print(\"The CSV file has no data rows.\")\n",
    "    else:   \n",
    "        ts = precip_time_series(f\"/nfs/a319/gy17m2a/Metrics/DanishRainData/{file}\")\n",
    "        if len(ts.data[ts.data['precipitation (mm/hr)']<0]) >0:\n",
    "            print(\"Not including, still has negatives\")\n",
    "            negatives.append(file)\n",
    "        else:\n",
    "            ts.pad_and_resample('5')\n",
    "\n",
    "            # check if enough values\n",
    "            dt_index = ts.data.index\n",
    "            full_range = pd.date_range(start=dt_index.min(), end=dt_index.max(), freq='5T')\n",
    "            missing = full_range.difference(dt_index)\n",
    "            print(f\"Number of missing time steps: {len(missing)}\")\n",
    "            if len(missing) > 0:\n",
    "                print(\"First few missing timestamps:\")\n",
    "                print(missing[:10])\n",
    "\n",
    "            analysis = rainfall_analysis('11h', ts)\n",
    "\n",
    "            if ts.events != None:\n",
    "                analysis.get_metrics()\n",
    "                df = pd.DataFrame(analysis.metrics)\n",
    "                df['gauge_num'] = file[:6]\n",
    "                all_events.append(df)\n",
    "                with open(f'/nfs/a319/gy17m2a/Metrics/DanishRainDataPickles/{file}.pkl', 'wb') as f:\n",
    "                    pickle.dump(ts, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "                all_events_df = pd.concat(all_events)\n",
    "                all_events_df.to_csv(\"All_events_new.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3449ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(all_events_df))\n",
    "all_events_df = all_events_df[all_events_df['total_precip']>4]\n",
    "print(len(all_events_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfe51d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ts.events != None:\n",
    "    analysis.get_metrics()\n",
    "    df = pd.DataFrame(analysis.metrics)\n",
    "    df['gauge_num'] = file[:6]\n",
    "    all_events.append(df)\n",
    "    print(len(all_events))\n",
    "    with open(f'/nfs/a319/gy17m2a/Metrics/DanishRainDataPickles/{file}.pkl', 'wb') as f:\n",
    "        pickle.dump(ts, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    all_events_df = pd.concat(all_events)\n",
    "    all_events_df.to_csv(\"All_events_new.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2696a47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_and_resample(ts, freq = f'{temp_res}min' ,pad_value = 0):\n",
    "    # Resample the data to the specified frequency and pad missing values with pad_value\n",
    "    ts.data = ts.data.resample(freq).sum().fillna(pad_value)\n",
    "    ts.data *= 60 / temp_res\n",
    "    ts.padded = True\n",
    "    return ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7590a68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "event = ts.raw_events[2]\n",
    "find_heaviest_run_half(event.iloc[:, 0].to_numpy(), '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff0db54d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making the class so I am\n",
      "5901\n",
      "duplicate flags length: 10\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pad_and_resample' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5386/2356694388.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprecip_time_series\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"/nfs/a319/gy17m2a/Metrics/DanishRainData_SVK/{file}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpad_and_resample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'5min'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# ts.data are values in mm/min they are 0.05, 0, 0, 0, 0.10. I want to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pad_and_resample' is not defined"
     ]
    }
   ],
   "source": [
    "ts = precip_time_series(f\"/nfs/a319/gy17m2a/Metrics/DanishRainData_SVK/{file}\")\n",
    "pad_and_resample(ts, '5min')\n",
    "ts.data[:20]\n",
    "# ts.data are values in mm/min they are 0.05, 0, 0, 0, 0.10. I want to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c64e505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(all_events_df))\n",
    "# all_events_df = all_events_df[all_events_df['total_precip']>4]\n",
    "# print(len(all_events_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a658b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del all_events_df['min_intensity']\n",
    "# del all_events_df['BSC']\n",
    "# del all_events_df['BSC_Index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d46c891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_events_df.to_csv(\"three_gauge_events.csv\", index=False)\n",
    "# all_events_df = pd.read_csv(\"three_gauge_events.csv\")\n",
    "# all_events_df.copy=all_events_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8f1a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del all_events_df['BSC']\n",
    "# del all_events_df['BSC_DMC']\n",
    "# del all_events_df['mean_intensity_DMC']\n",
    "# del all_events_df['min_intensity_DMC']\n",
    "# all_events_df.rename(columns={\"third_class_max_percent_DMCs\": \"third_class_max_percent_DMC\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc4a442",
   "metadata": {},
   "source": [
    "## Create Correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d7b455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparison_df = compare_metrics_from_df(all_events_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6a620e",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_events_df = all_events_df[all_events_df['duration'] % 20 == 0]\n",
    "comparison_df = compare_metrics_from_df(filtered_events_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f652e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(figsize=(10, 8))\n",
    "# metric = comparison_df['metric'][2]\n",
    "\n",
    "# x = all_events_df[metric]\n",
    "# y = all_events_df[f'{metric}_DMC']\n",
    "\n",
    "# # Remove NaNs\n",
    "# mask = ~x.isna() & ~y.isna()\n",
    "# x = x[mask]\n",
    "# y = y[mask]\n",
    "\n",
    "# # Density plot\n",
    "# sns.histplot(x=x, y=y, bins=5, pmax=0.9, ax=ax, cmap=\"viridis\", cbar=False)\n",
    "\n",
    "# ax.set_title(metric)\n",
    "# ax.set_xlabel('Raw')\n",
    "# ax.set_ylabel('DMC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0f8f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=8, ncols=6, figsize=(20, 15))\n",
    "\n",
    "for ax_num, ax in enumerate(axs.flatten()):\n",
    "    if ax_num < len(comparison_df['metric']):\n",
    "        metric = comparison_df['metric'][ax_num]\n",
    "        \n",
    "        x = all_events_df[metric]\n",
    "        y = all_events_df[f'{metric}_DMC']\n",
    "        \n",
    "        # Remove NaNs\n",
    "        mask = ~x.isna() & ~y.isna()\n",
    "        x = x[mask]\n",
    "        y = y[mask]\n",
    "        \n",
    "        if metric == '4th_w_peak':\n",
    "            bins=4\n",
    "        elif metric == '5th_w_peak':\n",
    "            bins=5   \n",
    "        elif metric == 'ARR_Thirds' or metric =='3rd_w_peak' or metric == 'third_class_max_percent':\n",
    "            bins=3\n",
    "        elif metric == 'BSC_Index':\n",
    "            bins=5\n",
    "        else:\n",
    "            bins=30\n",
    "        \n",
    "        # Density plot\n",
    "        sns.histplot(x=x, y=y, bins=bins, pmax=0.9, ax=ax, cmap=\"plasma\", cbar=False)\n",
    "        \n",
    "        ax.set_title(metric)\n",
    "        ax.set_xlabel('Raw')\n",
    "        ax.set_ylabel('DMC')\n",
    "    else:\n",
    "        ax.set_axis_off()\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.savefig(\"DMC_Raw_Comparison.png\", facecolor='white', edgecolor='white', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e015d1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_cols = []\n",
    "for col in all_events_df.columns:\n",
    "    if not col.endswith('_DMC'):\n",
    "        raw_cols.append(col)\n",
    "all_events_df_raw = all_events_df[raw_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ffd6bf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "correlation_matrix = all_events_df_raw.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d22e640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the plot\n",
    "plt.figure(figsize=(20, 15))\n",
    "\n",
    "# Mask the upper triangle\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "# Draw group separators (e.g., after every 3 variables)\n",
    "group_sizes = [0, 9, 5]  # Adjust based on your actual groupings\n",
    "group_cuts = np.cumsum(group_sizes)\n",
    "\n",
    "for cut in group_cuts[:-1]:  # Don't draw line after the last group\n",
    "    plt.axhline(cut, color='black', linewidth=1)\n",
    "    plt.axvline(cut, color='black', linewidth=1)\n",
    "\n",
    "# Create heatmap\n",
    "sns.heatmap(correlation_matrix, mask=mask, cmap='coolwarm', annot=False, fmt=\".2f\", \n",
    "            linewidths=0.5, cbar=True, xticklabels=True, yticklabels=True)\n",
    "\n",
    "# Improve layout\n",
    "plt.xticks(rotation=90)  # Rotate x-axis labels for readability\n",
    "plt.yticks(rotation=0)  # Ensure y-axis labels are readable\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9324bfe6",
   "metadata": {},
   "source": [
    "### Principal Components Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5c2fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9438f8cc",
   "metadata": {},
   "source": [
    "### PCA 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606914ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix.to_csv(\"correlation.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4e2a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'three_gauge_events.csv'\n",
    "df = all_events_df\n",
    "# Display the head of the dataframe to understand its structure\n",
    "\n",
    "# Standardizing the data before PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(df)\n",
    "\n",
    "# Performing PCA\n",
    "pca = PCA()\n",
    "pca.fit(scaled_data)\n",
    "\n",
    "# Explained variance ratio\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "\n",
    "# Plotting the explained variance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(explained_variance) + 1), explained_variance, marker='o')\n",
    "plt.title('Explained Variance by Principal Components')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Variance Explained')\n",
    "plt.xticks(range(1, len(explained_variance) + 1))\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Cumulative explained variance\n",
    "cumulative_variance = explained_variance.cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67352689",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting the explained variance ratio\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(1, len(explained_variance) + 1), explained_variance, alpha=0.7, color='b', label='Explained Variance Ratio')\n",
    "plt.plot(range(1, len(explained_variance) + 1), cumulative_variance, marker='o', color='r', label='Cumulative Explained Variance')\n",
    "plt.title('PCA Explained Variance Ratio and Cumulative Explained Variance')\n",
    "plt.xlabel('Principal Components')\n",
    "plt.ylabel('Variance Explained')\n",
    "plt.xticks(range(1, len(explained_variance) + 1))\n",
    "plt.axhline(y=0.80, color='g', linestyle='--', label='80% Threshold')\n",
    "plt.axhline(y=0.90, color='orange', linestyle='--', label='90% Threshold')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e362bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming we have the PCA components and the original feature names\n",
    "# Let's extract the features contributing to the first 6 principal components\n",
    "\n",
    "# Assuming pca.components_ contains the PCA components and original_feature_names is a list of feature names\n",
    "# For demonstration, let's create a mock PCA components array and feature names\n",
    "pca_components = pca.components_[:10]\n",
    "original_feature_names = all_events_df.columns\n",
    "\n",
    "# For now, let's create mock data for demonstration\n",
    "np.random.seed(0)\n",
    "# pca_components = np.random.rand(6, len(explained_variance))  # Mock PCA components for 6 components\n",
    "# original_feature_names = [f'Feature {i+1}' for i in range(len(explained_variance))]  # Mock feature names\n",
    "\n",
    "# Creating a DataFrame to show the contribution of each feature to the first 6 components\n",
    "feature_contributions = pd.DataFrame(pca_components, columns=original_feature_names)\n",
    "feature_contributions.index = [f'PC {i+1}' for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccaa0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarizing the most influential features for the first 6 principal components\n",
    "# Finding the top contributing features for each principal component\n",
    "num_top_features = 20  # Number of top features to display\n",
    "summary = {}\n",
    "\n",
    "for i in range(6):  # For each principal component\n",
    "    top_features = feature_contributions.iloc[i].nlargest(num_top_features)\n",
    "    summary[f'PC {i+1}'] = top_features.index.tolist()\n",
    "\n",
    "summary_df = pd.DataFrame(summary)\n",
    "\n",
    "# Displaying the summary of top features for each principal component\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa81d907",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Drop any non-numeric or unwanted columns (e.g., identifiers)\n",
    "X = all_events_df.select_dtypes(include=[float, int])\n",
    "\n",
    "# Standardize\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# PCA\n",
    "pca = PCA(n_components=min(X.shape[1], 10))  # adjust number of components as needed\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Create DataFrame of PCA results\n",
    "pca_df = pd.DataFrame(X_pca, columns=[f\"PC{i+1}\" for i in range(X_pca.shape[1])])\n",
    "pca_df.index = all_events_df.index\n",
    "\n",
    "# Variance explained\n",
    "explained_var = pca.explained_variance_ratio_\n",
    "\n",
    "# Scree plot\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.barplot(x=[f\"PC{i+1}\" for i in range(len(explained_var))], y=explained_var)\n",
    "plt.title(\"Explained Variance by Principal Component\")\n",
    "plt.ylabel(\"Proportion of Variance\")\n",
    "plt.xlabel(\"Principal Component\")\n",
    "plt.show()\n",
    "\n",
    "# Optional: biplot or loading plot\n",
    "loadings = pd.DataFrame(pca.components_.T, index=X.columns, columns=pca_df.columns)\n",
    "\n",
    "# Plot first two PCs with loadings (biplot style)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=pca_df[\"PC1\"], y=pca_df[\"PC2\"])\n",
    "for i, var in enumerate(X.columns):\n",
    "    plt.arrow(0, 0, loadings[\"PC1\"][i]*2, loadings[\"PC2\"][i]*2,\n",
    "              color='r', alpha=1)\n",
    "    plt.text(loadings[\"PC1\"][i]*2.2, loadings[\"PC2\"][i]*2.2, var, color='r')\n",
    "\n",
    "plt.title(\"PCA Biplot (PC1 vs PC2)\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.grid(True)\n",
    "plt.axhline(0, color='grey', linewidth=0.5)\n",
    "plt.axvline(0, color='grey', linewidth=0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3900327",
   "metadata": {},
   "source": [
    "### Hierarchical clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb417995",
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered = cluster_from_correlation1(correlation_matrix, num_clusters=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4af810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for clustering and silhouette score calculation\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Prepare to calculate silhouette scores for different numbers of clusters\n",
    "range_n_clusters = range(2, 11)  # Testing from 2 to 10 clusters\n",
    "silhouette_scores = []\n",
    "\n",
    "# Calculate silhouette scores for each number of clusters\n",
    "for n_clusters in range_n_clusters:\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(correlation_matrix)\n",
    "    silhouette_avg = silhouette_score(correlation_matrix, cluster_labels)\n",
    "    silhouette_scores.append(silhouette_avg)\n",
    "\n",
    "# Plot the silhouette scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range_n_clusters, silhouette_scores, marker='o')\n",
    "plt.title('Silhouette Scores for Different Numbers of Clusters')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.xticks(range_n_clusters)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a94523",
   "metadata": {},
   "source": [
    "### Correlations with moments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c118381c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your moment metrics of interest\n",
    "moments = ['mean_intensity', 'std', 'kurtosis', 'skewness']\n",
    "moments = [m for m in moments if m in all_events_df.columns]\n",
    "print(moments)\n",
    "corr_matrix, pval_matrix = corr_and_pvalues(all_events_df[5:10])\n",
    "\n",
    "# Set up plot\n",
    "fig, axs = plt.subplots(ncols=len(moments), figsize=(20,10))\n",
    "vmin, vmax = -1, 1\n",
    "heatmaps = []\n",
    "\n",
    "for ax, moment in zip(axs, moments):\n",
    "    # Correlations for one column\n",
    "    col_corr = corr_matrix[[moment]].sort_values(by=moment, ascending=False)\n",
    "    col_pval = pval_matrix[[moment]].loc[col_corr.index]\n",
    "\n",
    "    # Annotate with asterisks for significance\n",
    "    annotations = col_corr.copy()\n",
    "    for i in range(len(annotations)):\n",
    "        r = col_corr.iloc[i, 0]\n",
    "        p = col_pval.iloc[i, 0]\n",
    "        annotations.iloc[i, 0] = f\"{r:.2f}\" + (\"*\" if p < 0.05 else \"\")\n",
    "\n",
    "    # Draw heatmap without colorbar\n",
    "    hm = sns.heatmap(col_corr[1:], annot=annotations[1:], fmt=\"\", cmap='coolwarm',\n",
    "                     vmin=vmin, vmax=vmax, ax=ax, cbar=False, linewidths=0.5,\n",
    "                     linecolor='lightgray', annot_kws={\"size\": 10})\n",
    "\n",
    "    ax.set_title(f'Correlation with {moment}')\n",
    "    heatmaps.append(hm)\n",
    "\n",
    "# Add shared colorbar\n",
    "cbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])\n",
    "fig.colorbar(heatmaps[-1].collections[0], cax=cbar_ax)\n",
    "plt.tight_layout(rect=[0, 0, 0.9, 1])\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
